---
title: "Predicting Loan Defaults: A Data-Driven Approach to Credit Risk Analysis"
author: "Student Number - 720017170"
subtitle: BEE2041 - Data Science in Economics
format: pdf
toc: true
execute:
    echo: false
    warning: false
    message: false
    results: false
header-includes:
    - \usepackage{float}  
---

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as mpatches

from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.svm import SVC
import statsmodels.api as sm
from sklearn.utils import resample

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import log_loss

```

\newpage

## **1. Introduction**

Access to credit is a important driver of economic growth, allowing households or buisnesses to invest, expand and smooth consumption. However, credit risk remains a fundemental challenge for financial institutions, as loan defaulting can lead to substantial financial losses for both the company and for stakeholders. The ability to predict these defaults is vital for lending institutions to mitigate their risk and make more informed lending predictions. Recent advancements in machine learning (ML) have aided in the development of robust predictive models that outperform traditional credit-scoring methods (Yang, 2024)

Ensemble methods such as Random Forest (RF), XGBoost, and Light Gradient Boosting Machines (LGBM), have shown significant promise in improving classification accuracy over traditional statistical methods (Yadav, 2025). These models offer enhanced predictive capacity due to their ability to capture non-linear relationships in borrower data, providing financial institutions with more reliable risk assessment (Roy, 2025)

This study aims to explore a data-driven approach to credit risk analysis by using ML methods to predict loan defaulting. Logistic regression (LR), RF, XGBoost and LGBM have all been implemented and compared using standard performance metrics such as accuracy, precision, recall, F1-score and area under the curve (AUC). Moreover, exploratory data analysis will be conducted to examine the distribution of important financial variables, identify correlations and allow for optimised feature selection to improve model performance.

Due to the increasing reliance on alternative data sources and advanced computational methods in the financial sector, the results of this study may have significant practical implications. Improved credit risk analysis can help lenders reduce default rates, minimise losses and promote more inclusive access to credit (Ellsworth, 2025). By leveraging the latest ML methods, this project aims to contribute to the growing body of research on predictive analytics in finance and support more robust lending practices (Khoshkhoy Nilash & Esmaeilpour, 2025).

## **2. Data**

Prior to conducting the analysis of credit risk, we need to understand and organise the data. For this analysis we will be using a loan defaulting dataset from Kaggle (reference), consisting of 16 variables/columns and 255,347 observations.

### 2.1 Preparing the Data

```{python}
loan_data = pd.read_csv('credit_risk_dataset.csv')
# Data Cleaning
# Drop duplicate rows if any
loan_data.drop_duplicates(inplace=True)

# Handle missing values
loan_data.dropna(inplace=True)

# Calculate distribution before downsampling
default_counts_before = loan_data['loan_status'].value_counts()

# Balance the classes in the Default column
default_0 = loan_data[loan_data['loan_status'] == 0]
default_1 = loan_data[loan_data['loan_status'] == 1]

# Downsample majority class
default_0_downsampled = resample(default_0, 
                                 replace=False,    
                                 n_samples=len(default_1),  
                                 random_state=123)

# Combine minority class with downsampled majority class
loan_data_balanced = pd.concat([default_0_downsampled, default_1])

# Calculate distribution after downsampling
default_counts_after = loan_data_balanced['loan_status'].value_counts()

# Reduce the sample size to 1% of the balanced dataset while maintaining class proportions
loan_data_reduced, _ = train_test_split(loan_data_balanced, test_size=0.75, stratify=loan_data_balanced['loan_status'], random_state=123)

# Create a DataFrame for plotting
distribution_df = pd.DataFrame({
    'Before Downsampling': default_counts_before,
    'After Downsampling': default_counts_after
}).reset_index().melt(id_vars='loan_status', var_name='Stage', value_name='Count')
```

```{python}
# Create a DataFrame with variable names and data types
variable_info = pd.DataFrame({
    'Variable': loan_data.columns,
    'Data Type': loan_data.dtypes.astype(str)
})

# Add definitions for each variable
variable_info['Definition'] = [
    'Age of the borrower',
    'Income of the borrower',
    'Loan amount requested by the borrower',
    'Credit score of the borrower',
    'Number of months the borrower has been employed',
    'Number of credit lines the borrower has',
    'Interest rate of the loan',
    'Term of the loan in months',
    'Debt-to-Income ratio of the borrower',
    'Education level of the borrower',
    'Employment type of the borrower',
    'Marital status of the borrower'
]

# Convert the DataFrame to LaTeX format with appropriate formatting
variable_info_latex = variable_info.to_latex(index=False,
                                             caption="Variable Information",
                                             label="Table 1:variable_info",
                                             column_format="lll",
                                             escape=False)
variable_info_latex = variable_info_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")


# Save to a LaTeX file
with open("variable_info_table.tex", "w") as f:
    f.write(variable_info_latex)
```

\input{variable_info_table.tex}

### **2.1 Descriptive Statistics**

```{python}
# Compute summary statistics
summary_stats = loan_data.describe().transpose()
summary_stats = summary_stats[['count', 'mean', '50%', 'std', 'min', 'max']]
summary_stats.columns = ['N', 'Mean', 'Median', 'SD', 'Min', 'Max']
summary_stats.index.name = "Variable"

# Round values for better readability and format as strings for LaTeX output
summary_stats = summary_stats.round(1).astype(str)

# Convert index to column for better formatting
summary_stats.reset_index(inplace=True)

# Convert table to LaTeX format with formatting
latex_table = summary_stats.to_latex(index=False,
                                     caption="Summary Statistics of Numeric Variables",
                                     label="Table 2:summary_stats",
                                     column_format="lrrrrrr",
                                     escape=False)

latex_table = latex_table.replace("\\begin{table}", "\\begin{table}[H]\\centering")

# Save to a LaTeX file
with open("summary_table.tex", "w") as f:
    f.write(latex_table)
```

\input{summary_table.tex}

### **2.2 Distribution Analysis**

```{python}
# Set Loan Status to Categorical

loan_data = loan_data_reduced
loan_data['loan_status'] = loan_data['loan_status'].astype('category')

numeric_cols = loan_data.select_dtypes(include=[np.number]).columns
num_cols = 4
num_rows = int(np.ceil(len(numeric_cols) / num_cols))

plt.rcParams.update({'font.size': 75})
plt.figure(figsize=(80, 25 * num_rows))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(num_rows, num_cols, i)
    loan_data[col].hist(bins=30, edgecolor='black')
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)  # Tilt the x-axis labels by 45 degrees

plt.tight_layout()
plt.figtext(0.5, -0.01, "Figure ?: Histograms of all Numeric Variables", ha="center", fontsize=90)
plt.show()
plt.rcParams.update({'font.size': 14})

```

```{python}
#Clearning pt 2
# Convert string variables using Label Encoding into categorical
categorical_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    loan_data[col] = le.fit_transform(loan_data[col])
    label_encoders[col] = le
```

```{python}
# Box plots for all numeric variables pre normalisation
plt.rcParams.update({'font.size': 12})
plt.figure(figsize=(12, 5))
loan_data.boxplot()
plt.figtext(0.5, -0.2, "Figure ?: Box Plots of All Variables Before Normalisation", ha="center", fontsize=11)
plt.xticks(rotation=45)
plt.show()
```

```{python}
#Normalisation using z-score normalisation
numeric_cols = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
scaler = StandardScaler()
loan_data[numeric_cols] = scaler.fit_transform(loan_data[numeric_cols])

# Box plots for all numeric variables post normalisation
plt.figure(figsize=(12,5))
loan_data.boxplot()
plt.figtext(0.5, -0.2, "Figure ?: Box Plots of All Variables After Normalisation", ha="center", fontsize=11)
plt.xticks(rotation=45)
plt.show()
```

```{python}
# Plot the stacked bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Stage', y='Count', hue='loan_status', data=distribution_df, palette='gray')
plt.xlabel('Stage')
plt.ylabel('Count')
plt.figtext(0.5, -0.01, "Figure ?: Distribution of Default Before and After Downsampling", ha="center", fontsize=11)
plt.legend(title='Default', loc='upper right')
plt.show()
```

Downsampled the dataset to ensure that the models didn't get affected by the magnitude of the majority class = can affect performance metrics. Also, reduced the size of the dataset by 75% in order to decrease the execution time to be reasonable.

### **2.3 Correlation Analysis**

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Correlation plot
plt.figure(figsize=(12, 10))
sns.heatmap(loan_data.corr(method='spearman'), annot=True, cmap='Greys', fmt='.2f', linewidths=0.5)
plt.figtext(0.5, -0.09, "Figure ?: Correlation Plot of All Variables ", ha="center", fontsize=11)
plt.show()

# VIF values
train_data, test_data = train_test_split(loan_data, test_size=0.2, random_state=123)
X_train = train_data.drop(columns=['loan_status'])

# Add a constant term for intercept
X_train_vif = sm.add_constant(X_train)

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X_train_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_train_vif.values, i) for i in range(X_train_vif.shape[1])]

# Drop the constant term from the VIF DataFrame
vif_data = vif_data[vif_data["Feature"] != "const"]

# Convert the DataFrame to LaTeX format with appropriate formatting
vif_latex = vif_data.to_latex(index=False,
                              caption="Variance Inflation Factor (VIF) Values",
                              label="Table 4:vif_values",
                              column_format="lc",
                              escape=False)
vif_latex = vif_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")
vif_latex = vif_latex.replace("\\end{table}", "\\end{table}")
# Save to a LaTeX file
with open("vif_table.tex", "w") as f:
    f.write(vif_latex)
```

\input{vif_table.tex}

Selected Features with a magnitude of correlation to class above 0.05 to remove any variables with low correlation likely to reduce predictive performance

## **3. Results and Discussion**

```{python}
# Split the data into test and train
train_data, test_data = train_test_split(loan_data, test_size=0.2, random_state=123)
# Prepare dataset
X_train = train_data.drop(columns=['loan_status'])
y_train = train_data['loan_status']
X_test = test_data.drop(columns=['loan_status'])
y_test = test_data['loan_status']
```

### **3.1 Logistic Regression**

```{python}
import sys
import os
sys.stdout = open(os.devnull, 'w')

# Define the parameter grid for Grid Search
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

# Initialize Grid Search with Cross-Validation for Logistic Regression
stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_lr_model = grid_search.best_estimator_
best_lr_model.fit(X_train, y_train)

sys.stdout = sys.__stdout__

# Make Predictions
lr_predictions = best_lr_model.predict(X_test)
lr_probabilities = best_lr_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_lr = confusion_matrix(y_test, lr_predictions)

# Compute Performance Metrics
accuracyLR = round(accuracy_score(y_test, lr_predictions), 3)
precisionLR = round(precision_score(y_test, lr_predictions), 3)
recallLR = round(recall_score(y_test, lr_predictions), 3)
f1_scoreLR = round(f1_score(y_test, lr_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probabilities)
auc_value_LR = round(auc(fpr_lr, tpr_lr), 3)
```

```{python}
# Plot ROC Curve for Logistic Regression
plt.figure(figsize=(8, 5))
plt.plot(fpr_lr, tpr_lr, color="black", linewidth=2, label=f"AUC: {auc_value_LR}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.figtext(0.5, -0.1, "Figure ?: ROC Curve for Logistic Regression Mod", ha="center", fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_lr = pd.DataFrame(conf_matrix_lr, index=["No Default", "Default"],
                          columns=["No Default", "Default"])

# Plot Confusion Matrix for Logistic Regression
plt.figure(figsize=(8, 5))
sns.heatmap(conf_df_lr, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.figtext(0.5, -0.1, "Figure ?: Confusion Matrix for Logistic Regression Model", ha="center", fontsize=11)
plt.show()
```

```{python}
# Fit logistic regression model using statsmodels for odds ratios
X_train_sm = sm.add_constant(X_train)  # Add constant term for intercept
logit_model = sm.Logit(y_train, X_train_sm).fit()
lr_train = best_lr_model.predict_proba(X_train)[:, 1]  # Extract probability for positive class


# Extract odds ratios and 95% confidence intervals
odds_ratios = np.exp(logit_model.params)
conf = np.exp(logit_model.conf_int())
conf['OR'] = odds_ratios
conf.columns = ['2.5%', '97.5%', 'OR']
conf = conf.reindex((conf['OR'] - 1).abs().sort_values(ascending=True).index)
conf = conf.drop(['const'])

# Plot odds ratios as a forest plot with 95% confidence intervals
plt.figure(figsize=(8, 5))
plt.errorbar(conf['OR'], conf.index, xerr=[conf['OR'] - conf['2.5%'], conf['97.5%'] - conf['OR']], fmt='o', color='black', ecolor='gray', capsize=3)
plt.axvline(x=1, linestyle='--', color='red')
plt.xlabel('Odds Ratio')
plt.ylabel('Features')
plt.figtext(0.5, -0.1, "Figure ?: Odds Ratios with 95% Confidence Intervals", ha="center", fontsize=11)
plt.show()
```

### **3.2 Random Forest**

```{python}
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [2,4],
    'bootstrap': [True, False]
}

# Define Stratified K-Fold
stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Initialize Grid Search with Cross-Validation
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)

# Make Predictions
rf_predictions = best_rf_model.predict(X_test)
rf_probabilities = best_rf_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_rf = confusion_matrix(y_test, rf_predictions)

# Compute Performance Metrics
accuracyRF = round(accuracy_score(y_test, rf_predictions), 3)
precisionRF = round(precision_score(y_test, rf_predictions), 3)
recallRF = round(recall_score(y_test, rf_predictions), 3)
f1_scoreRF = round(f1_score(y_test, rf_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probabilities)
auc_value_RF = round(auc(fpr_rf, tpr_rf), 3)

```

```{python}
# Plot ROC Curve for Random Forest
plt.figure(figsize=(8, 5))
plt.plot(fpr_rf, tpr_rf, color="black", linewidth=2, label=f"AUC: {auc_value_RF}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for Random Forest Model", y = 0.0005, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_rf = pd.DataFrame(conf_matrix_rf, index=["No Default", "Default"],
                          columns=["No Default", "Default"])

# Plot Confusion Matrix for Random Forest
plt.figure(figsize=(8, 5))
sns.heatmap(conf_df_rf, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for Random Forest Model", y = 0.0005, fontsize=11)
plt.show()
```

```{python}
# Importance values
# Extract feature importances
feature_importances = best_rf_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(8, 5))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: Feature Importances from Random Forest Model", y = 0.0005, fontsize=11)
plt.show()
```

### **3.3 XGBoost**

```{python}
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}

grid_search = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_xgb_model = grid_search.best_estimator_
best_xgb_model.fit(X_train, y_train)

# Make Predictions
xgb_predictions = best_xgb_model.predict(X_test)
xgb_probabilities = best_xgb_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_xgb = confusion_matrix(y_test, xgb_predictions)

# Compute Performance Metrics
accuracyXGB = round(accuracy_score(y_test, xgb_predictions), 3)
precisionXGB = round(precision_score(y_test, xgb_predictions), 3)
recallXGB = round(recall_score(y_test, xgb_predictions), 3)
f1_scoreXGB = round(f1_score(y_test, xgb_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probabilities)
auc_value_XGB = round(auc(fpr_xgb, tpr_xgb), 3)

```

```{python}
# Plot ROC Curve for XGBoost
plt.figure(figsize=(8, 5))
plt.plot(fpr_xgb, tpr_xgb, color="black", linewidth=2, label=f"AUC: {auc_value_XGB}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for XGBoost Model", y = 0.0005, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df = pd.DataFrame(conf_matrix_xgb, index=["No Default", "Default"],
                       columns=["No Default", "Default"])

# Plot Confusion Matrix
plt.figure(figsize=(8, 5))
sns.heatmap(conf_df, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for XGBoost Model", y = 0.0005, fontsize=11)
plt.show()
```

```{python}
# Extract feature importances
feature_importances = best_xgb_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: eature Importances from XGBoost Model", y = 0.0005, fontsize=11)
plt.show()
```

### **3.4  Light Gradient Boosted Machine**

```{python}
from lightgbm import LGBMClassifier

# Build the neural network model
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1],
    'num_leaves': [20, 31],
    'subsample': [0.8, 1.0]
}

lgbm_params = {'verbosity': -1}  # Set LightGBM verbosity to -1 (no warnings)

# Initialize Grid Search with Cross-Validation for LGBMClassifier
grid_search = GridSearchCV(LGBMClassifier(**lgbm_params,random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_lgbm_model = grid_search.best_estimator_
best_lgbm_model.fit(X_train, y_train)

# Make Predictions
lgbm_predictions = best_lgbm_model.predict(X_test)
lgbm_probabilities = best_lgbm_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_lgbm = confusion_matrix(y_test, lgbm_predictions)

# Compute Performance Metrics
accuracyLGBM = round(accuracy_score(y_test, lgbm_predictions), 3)
precisionLGBM = round(precision_score(y_test, lgbm_predictions), 3)
recallLGBM = round(recall_score(y_test, lgbm_predictions), 3)
f1_scoreLGBM = round(f1_score(y_test, lgbm_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_lgbm, tpr_lgbm, _ = roc_curve(y_test, lgbm_probabilities)
auc_value_LGBM = round(auc(fpr_lgbm, tpr_lgbm), 3)
```

```{python}

# Plot ROC Curve for LightGBM
plt.figure(figsize=(8, 5))
plt.plot(fpr_lgbm, tpr_lgbm, color="black", linewidth=2, label=f"AUC: {auc_value_LGBM}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for LightGBM Model", y = 0.0005, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_lgbm = pd.DataFrame(conf_matrix_lgbm, index=["No Default", "Default"],
                            columns=["No Default", "Default"])

# Plot Confusion Matrix for LightGBM
plt.figure(figsize=(8, 5))
sns.heatmap(conf_df_lgbm, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for LightGBM Model", y = 0.0005, fontsize=11)
plt.show()

```

```{python}
# Extract feature importances
feature_importances = best_lgbm_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances in black and white with black outlines
plt.figure(figsize=(8, 5))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: Feature Importances from LightGBM Model", y = 0.0005, fontsize=11)
plt.show()
```

### **3.5 Model Evaluation and Comparisons**

```{python}
# Create Accuracy DataFrame
accuracy_df = pd.DataFrame({
    "Model": ["LR","RF", "XGB", "LGBM"],
    "Value": [accuracyLR,accuracyRF, accuracyXGB, accuracyLGBM]
})

# Create Bar Plot for Accuracy
plt.figure(figsize=(8, 3))
sns.barplot(x="Model", y="Value", data=accuracy_df, palette="gray", edgecolor="black")
for index, row in accuracy_df.iterrows():
    plt.text(index, row.Value + 0.02, round(row.Value, 3), ha="center", fontsize=10)
plt.ylim(0, 1)
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.figtext(0.5, -0.1, "Figure ?: Accuracy for Each Model", ha="center", fontsize=11)
plt.show()

```


```{python}


log_loss_LR = log_loss(y_test, lr_predictions)
log_loss_RF = log_loss(y_test, rf_predictions)
log_loss_XGB = log_loss(y_test, xgb_predictions)
log_loss_LGBM = log_loss(y_test, lgbm_predictions)

# Create a DataFrame for Performance Metrics
performance_df = pd.DataFrame({
    "Model": ["LR","RF", "XGB", "LGBM"],
    "Accuracy": [accuracyLR,accuracyRF, accuracyXGB, accuracyLGBM],
    "Precision": [precisionLR,precisionRF, precisionXGB, precisionLGBM],
    "Recall": [recallLR,recallRF, recallXGB, recallLGBM],
    "F1 Score": [f1_scoreLR,f1_scoreRF, f1_scoreXGB, f1_scoreLGBM],
    "AUC": [auc_value_LR,auc_value_RF, auc_value_XGB, auc_value_LGBM],
    "Log Loss": [log_loss_LR,log_loss_RF, log_loss_XGB, log_loss_LGBM]
})

# Round values for better readability and format as strings for LaTeX output
performance_df = performance_df.round(3).astype(str)

# Convert the DataFrame to LaTeX format with appropriate formatting
performance_latex = performance_df.to_latex(index=False,
                                             caption="Performance Metrics for Each Model",
                                             label="Table 3 :performance_metrics",
                                             column_format="lrrrrrr",
                                             escape=False)

# Replace underscores with LaTeX-safe versions
performance_latex = performance_latex.replace("AUC", "AUC")
performance_latex = performance_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")

# Save to a LaTeX file
with open("performance_table.tex", "w") as f:
    f.write(performance_latex)
```

\input{performance_table.tex}

## **4. Conclusion**

[Link to Github Repository = https://github.com/JoshLG18/DSE-EMP-Project](https://github.com/JoshLG18/DSE-EMP-Project)