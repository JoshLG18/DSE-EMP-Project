---
title: "Predicting Loan Defaults: A Data-Driven Approach to Credit Risk Analysis"
author: "Student Number - 720017170"
subtitle: BEE2041 - Data Science in Economics
format: pdf
toc: true
execute:
    echo: false
    warning: false
    message: false
    results: false
header-includes:
    - \usepackage{float}  
---

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as mpatches

from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.svm import SVC
import statsmodels.api as sm
from sklearn.utils import resample
from sklearn.metrics import log_loss
from sklearn.metrics import brier_score_loss
from sklearn.metrics import precision_recall_curve, roc_curve, f1_score
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LinearRegression
```

\newpage

## **1. Introduction**

Access to credit is a important driver of economic growth, allowing households or businesses to invest, expand and smooth consumption. However, credit risk remains a fundamental challenge for financial institutions, as loan defaulting can lead to substantial financial losses for both the company and stakeholders. The ability to predict these defaults is vital for lending institutions to mitigate their risk and make more informed lending predictions. Recent advancements in machine learning (ML) have aided in the development of robust predictive models that outperform traditional credit-scoring methods (Yang, 2024)

Ensemble methods such as Random Forest (RF), XGBoost, and Light Gradient Boosting Machines (LGBM), have shown significant promise in improving classification accuracy over traditional statistical methods (Yadav, 2025). These models offer enhanced predictive capacity due to their ability to capture non-linear relationships in borrower data, providing financial institutions with more reliable risk assessment (Roy, 2025)

This study aims to explore a data-driven approach to credit risk analysis by using ML methods to predict loan defaulting. Logistic regression (LR), RF, XGBoost and LGBM have all been implemented and compared using standard performance metrics such as accuracy, precision, recall, F1-score and area under the curve (AUC). Moreover, exploratory data analysis will be conducted to examine the distribution of important financial variables, identify correlations and allow for optimised feature selection to improve model performance.

Due to the increasing reliance on alternative data sources and advanced computational methods in the financial sector, the results of this study may have significant practical implications. Improved credit risk analysis can help lenders reduce default rates, minimise losses and promote more inclusive access to credit (Ellsworth, 2025). By leveraging the latest ML methods, this project aims to contribute to the growing body of research on predictive analytics in finance and support more robust lending practices (Khoshkhoy Nilash & Esmaeilpour, 2025).

## **2. Data**

Prior to conducting the analysis of credit risk, we need to understand and organise the data. For this analysis we will be using a loan defaulting dataset from Kaggle (reference), consisting of 12 variables/columns and 28,501 observations, illustrated in Table 1.

```{python}
loan_data = pd.read_csv('credit_risk_dataset.csv')

loan_data.columns = ['PersonAge', 'PersonIncome', 'PersonHomeOwnership', 'PersonEmpLength', 'LoanIntent', 'LoanGrade', 'LoanAmnt', 'LoanIntRate', 'LoanStatus', 'LoanPercentIncome', 'PreviousDefault', 'CredHistory']

# Data Cleaning

# Drop duplicate rows if any
loan_data.drop_duplicates(inplace=True)

# Handle missing values
# Calculate the number of missing values for each column
missing_values = loan_data.isnull().sum()

# Create a DataFrame for missing values
missing_values_df = pd.DataFrame({
    'Variable': missing_values.index,
    'Missing Values': missing_values.values
})

# Convert the DataFrame to LaTeX format with appropriate formatting
missing_values_latex = missing_values_df.to_latex(index=False,
                                                  caption="Missing Values in Each Variable",
                                                  label="Table:missing_values",
                                                  column_format="lc",
                                                  escape=False)

missing_values_latex = missing_values_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")

# Save to a LaTeX file
with open("missing_values_table.tex", "w") as f:
    f.write(missing_values_latex)


# Replacing emplength with median due to skew
loan_data["PersonEmpLength"].fillna(loan_data["PersonEmpLength"].median(), inplace=True)

# Regression imputation for loanintrate as corr with loangrade

    # Convert LoanGrade to a categorical variable
loan_data['LoanGrade'] = loan_data['LoanGrade'].astype('category')

    # Label encode the LoanGrade column
label_encoder = LabelEncoder()
loan_data['LoanGrade'] = label_encoder.fit_transform(loan_data['LoanGrade'])

# Separate train (non-missing LoanIntRate) and test (missing LoanIntRate)
train_data = loan_data.dropna(subset=["LoanIntRate"])
test_data = loan_data[loan_data["LoanIntRate"].isna()]  # Data where LoanIntRate is missing

# Train a Linear Regression model to predict LoanIntRate
model = LinearRegression()
model.fit(train_data[["LoanGrade"]], train_data["LoanIntRate"])

# Predict missing LoanIntRate values using LoanGrade
loan_data.loc[loan_data["LoanIntRate"].isna(), "LoanIntRate"] = model.predict(test_data[["LoanGrade"]])
```

```{python}
# Create a DataFrame with variable names and data types
variable_info = pd.DataFrame({
    'Variable': loan_data.columns,
    'Data Type': loan_data.dtypes.astype(str)
})

# Add definitions for each variable
variable_info['Definition'] = [
    'Age of the borrower',
    'Income of the borrower',
    'Home ownership of the borrower',
    'Employment length of the borrower',
    'Intention of the loan',
    'Loan grade',
    'Amount of the loan (USD)',
    'Loan interest rate',
    'Loan status (0 - not defaulted, 1 - defaulted)',
    'Loan percentage of income',
    'If the borrower has defaulted before',
    'Credit history length'
]

# Convert the DataFrame to LaTeX format with appropriate formatting
variable_info_latex = variable_info.to_latex(index=False,
                                             caption="Variable Information",
                                             label="Table 1:variable_info",
                                             column_format="lll",
                                             escape=False)

variable_info_latex = variable_info_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")

# Save to a LaTeX file
with open("variable_info_table.tex", "w") as f:
    f.write(variable_info_latex)
```

\input{variable_info_table.tex}

### **2.1 Preparing the Data**

\input{missing_values_table.tex}

Table 2 displays the missing values within the dataset for each variable. The only variables with missing data are *PersonEmpLength* and *LoaanIntRate*, containing 887 and 3095 observations with no values, respectively. Missing data can have a large impact on data analysis if not handled properly and can lead to skewed or incorrect conclusions, making handling this data in the correct way crucial. Due to the negatively skewed nature of *PersonEmpLength*, illustrated in Figure 1, median imputation was deployed in order to maintain the observations and not impact sample size. *LoanIntRate* saw a high correlation with *LoanGrade*, shown by Figure (? Corr Matrix), therefore regression imputation was used to fill these missing variables and not lose sample size.

### **2.2 Descriptive Statistics**

```{python}
# Compute summary statistics
summary_stats = loan_data.describe().transpose()
summary_stats = summary_stats[['count', 'mean', '50%', 'std', 'min', 'max']]
summary_stats.columns = ['N', 'Mean', 'Median', 'SD', 'Min', 'Max']
summary_stats.index.name = "Variable"

# Round values for better readability and format as strings for LaTeX output
summary_stats = summary_stats.round(1).astype(str)

# Convert index to column for better formatting
summary_stats.reset_index(inplace=True)

# Convert table to LaTeX format with formatting
latex_table = summary_stats.to_latex(index=False,
                                     caption="Summary Statistics of Numeric Variables",
                                     label="Table 2:summary_stats",
                                     column_format="lrrrrrr",
                                     escape=False)

latex_table = latex_table.replace("\\begin{table}", "\\begin{table}[H]\\centering")

# Save to a LaTeX file
with open("summary_table.tex", "w") as f:
    f.write(latex_table)

loan_data = loan_data[loan_data['PersonAge'] < 123]
loan_data = loan_data[loan_data['PersonEmpLength'] < 123]

```

\input{summary_table.tex}

Table 3 contains all the summary statistics for all variables within the dataset. *PersonAge* and *PersonEmpLength* show maximum values of 144 and 123 years respectively, which are both above the oldest age a person has lived (122 years), meaning that they are potential errors. To remove these errors from the dataset, both observations for *PersonEmpLength* were removed as to not impact the models. For *PersonAge*, all observations with ages above 122 years were removed. This left *PersonAge* with a maximum value of 94 and *PersonEmpLength* with a maximum value of 41, both which are reasonable.

### **2.3 Distribution Analysis**

```{python}
# Set Loan Status to Categorical

loan_data['LoanStatus'] = loan_data['LoanStatus'].astype('category')

numeric_cols = loan_data.select_dtypes(include=[np.number]).columns
numeric_cols = numeric_cols.drop(['LoanGrade'])
num_cols = 2
num_rows = int(np.ceil(len(numeric_cols) / num_cols))

plt.rcParams.update({'font.size': 75})
plt.figure(figsize=(80, 20 * num_rows))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(num_rows, num_cols, i)
    loan_data[col].hist(bins=30, edgecolor='black')
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)  # Tilt the x-axis labels by 45 degrees

plt.tight_layout()
plt.figtext(0.5, -0.01, "Figure ?: Histograms of all Numeric Variables", ha="center", fontsize=90)
plt.show()
plt.rcParams.update({'font.size': 14})
```

The histograms shown in Figure 1 illustrate the distributions for each numeric variable. All of the variables shown have negatively skewed distributions. This is due to individuals with low age likely to have low values in each of these variables. *PersonAge*,  *PersonEmpLength* and *CredLength* have very similar distributions, indicating potential correlation between these variables.

```{python}
#Clearning pt 2
# Convert string variables using Label Encoding into categorical
categorical_cols = ['PersonHomeOwnership', 'LoanIntent', 'PreviousDefault']
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    loan_data[col] = le.fit_transform(loan_data[col])
    label_encoders[col] = le
```

```{python}
# Box plots for all numeric variables pre normalisation
plt.rcParams.update({'font.size': 12})
plt.figure(figsize=(12, 4))
loan_data.boxplot()
plt.figtext(0.5, -0.4, "Figure ?: Box Plots of All Variables Before Normalisation", ha="center", fontsize=11)
plt.xticks(rotation=45)
plt.show()
```

Figure 3 shows that the data isn't scaled proportionally, therefore we need to apply a scaling technique. Due to the skewness of all the variables quantile transformation was deployed, normalised data is shown in Figure 4. The plot shows outliers, however there is no reason for these to be errors meaning they will not be removed. For example, the reason for outliers in *PersonIncome* is due to people earning considerably more than average.

```{python}
#Normalisation using z-score normalisation
numeric_cols = ['PersonAge', 'PersonIncome', 'PersonHomeOwnership', 'PersonEmpLength', 'LoanIntent', 'LoanGrade', 'LoanAmnt', 'LoanIntRate', 'LoanPercentIncome', 'PreviousDefault', 'CredHistory']
from sklearn.preprocessing import QuantileTransformer

# Apply Quantile Transformation = due to skewness of the data
scaler = QuantileTransformer(output_distribution='normal')
loan_data[numeric_cols] = scaler.fit_transform(loan_data[numeric_cols])

# Box plots for all numeric variables post normalisation
plt.figure(figsize=(12,4))
loan_data.boxplot()
plt.figtext(0.5, -0.4, "Figure ?: Box Plots of All Variables After Normalisation", ha="center", fontsize=11)
plt.xticks(rotation=45)
plt.show()
```



```{python}
# Calculate distribution before downsampling
default_counts_before = loan_data['LoanStatus'].value_counts()

# Balance the classes in the Default column
default_0 = loan_data[loan_data['LoanStatus'] == 0]
default_1 = loan_data[loan_data['LoanStatus'] == 1]

# Downsample majority class
default_0_downsampled = resample(default_0, 
                                 replace=False,    
                                 n_samples=len(default_1),  
                                 random_state=123)

# Combine minority class with downsampled majority class
loan_data_balanced = pd.concat([default_0_downsampled, default_1])

# Calculate distribution after downsampling
default_counts_after = loan_data_balanced['LoanStatus'].value_counts()

# Create a DataFrame for plotting
distribution_df = pd.DataFrame({
    'Before Downsampling': default_counts_before,
    'After Downsampling': default_counts_after
}).reset_index().melt(id_vars='LoanStatus', var_name='Stage', value_name='Count')

loan_data = loan_data_balanced
```

```{python}
# Plot the stacked bar plot
plt.figure(figsize=(10, 8))
sns.barplot(x='Stage', y='Count',hue = 'LoanStatus', data=distribution_df, palette='gray')
plt.xlabel('Stage')
plt.ylabel('Count')
plt.figtext(0.5, -0.03, "Figure ?: Distribution of LoanStatus Before and After Downsampling", ha="center", fontsize=11)
plt.legend(title='Default', loc='upper right')
plt.show()
```

Figure 5 demonstrates the the distribution of *LoanStatus*. Before downsampling there was a large discrepancy between the number of people who defaulted and who didn't. This can cause large impacts on the ML models deployed in the analysis, leading to skewed perforamnce metrics as the models will predict the majority class with high accuracy but the minority class with lower accuracy. To circumvent this issue, downsampling was performed to ensure both outcomes had the same number of observations, shown in Figure 6

### **2.4 Correlation Analysis**

```{python}
# Split the data into test and train
train_data, test_data = train_test_split(loan_data, test_size=0.2, random_state=123)
# Prepare dataset
X_train = train_data.drop(columns=['LoanStatus'])
y_train = train_data['LoanStatus']
X_test = test_data.drop(columns=['LoanStatus'])
y_test = test_data['LoanStatus']
```

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor

plt.rcParams.update({'font.size': 24})

# Correlation plot
plt.figure(figsize=(20, 15))
sns.heatmap(loan_data.corr(method='spearman'), annot=True, cmap='Greys', fmt='.2f', linewidths=0.5)
plt.figtext(0.5, -0.20, "Figure ?: Correlation Plot of All Variables ", ha="center", fontsize=26)
plt.show()

plt.rcParams.update({'font.size': 14})

# Add a constant term for intercept
X_train_vif = sm.add_constant(X_train)

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X_train_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_train_vif.values, i) for i in range(X_train_vif.shape[1])]

vif_data["VIF"] = vif_data["VIF"].astype(float).round(3)

# Drop the constant term from the VIF DataFrame
vif_data = vif_data[vif_data["Feature"] != "const"]

# Convert the DataFrame to LaTeX format with appropriate formatting
vif_latex = vif_data.to_latex(index=False,
                              caption="Variance Inflation Factor (VIF) Values",
                              label="Table 4:vif_values",
                              column_format="lc",
                              escape=False)
vif_latex = vif_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")
vif_latex = vif_latex.replace("\\end{table}", "\\end{table}")
# Save to a LaTeX file
with open("vif_table.tex", "w") as f:
    f.write(vif_latex)
```

Figure 7 shows a correlation plot quantifying the relationships between the variables and to the target *LoanStatus*. *LoanGrade* and *LoanIntRate* have a high correlation coefficient (0.96), indicating that they are highly correlated. Also, a similar relationship is shown between *PersonAge* and *CredHistory* (r = 0.80). Both these relationships make logical sense as someone who is older who have a longer credit history and as loan grade increases it is likley that the interest rate does as well. Due to the mullticolliearity in the data, these variables may have to be removed however, futher analysis with variance inflation factor (VIF) is required. 

\input{vif_table.tex}

VIF values for all the variables are shown within Table 4. In contrast to Figure 7, *LoanGrade*, *LoanIntRate*, *PersonAge*, *CredHistory* have low VIF values, indicating low levels of multicollinearity. However, *LoanAmnt* and *LoanPercentIncome* have VIF values greater than 10 which shows multicollinearity and actions need to be taken to ensure they don't affect the models. For the logistic regression, L1 and L2 regularisation was deployed to reduce the affects of multicollinearity. Due to the other models being tree based why handle multicollinearity well, therefore no futher processing is needed. 

Within this analysis, LR, RF, XGboost and LGBM models will be trained to predict *LoanStatus* using *PersonAge*, *PersonIncome*, *PersonHomeOwnership*, *PersonEmpLength*, *LoanIntent*, *LoanGrade*, *LoanAmnt*, *LoanIntRate*, *LoanPercentIncome*, *PreviousDefault* and *CredHistory*.

## **3. Results and Discussion**

### **3.1 Logistic Regression**

The first model deployed was an LR trained on all the standard variables, this model acts as a baseline to compare all more complex models with. 

```{python}
import sys
import os
sys.stdout = open(os.devnull, 'w')

# Define the parameter grid for Grid Search
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

# Initialize Grid Search with Cross-Validation for Logistic Regression
stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)

grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_lr_model = grid_search.best_estimator_
best_lr_model.fit(X_train, y_train)

sys.stdout = sys.__stdout__

# Make Predictions
lr_predictions = best_lr_model.predict(X_test)
lr_probabilities = best_lr_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_lr = confusion_matrix(y_test, lr_predictions)

# Compute Performance Metrics
accuracyLR = round(accuracy_score(y_test, lr_predictions), 3)
precisionLR = round(precision_score(y_test, lr_predictions), 3)
recallLR = round(recall_score(y_test, lr_predictions), 3)
f1_scoreLR = round(f1_score(y_test, lr_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probabilities)
auc_value_LR = round(auc(fpr_lr, tpr_lr), 3)
```

```{python}
# Plot ROC Curve for Logistic Regression
plt.figure(figsize=(8, 4))
plt.plot(fpr_lr, tpr_lr, color="black", linewidth=2, label=f"AUC: {auc_value_LR}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.figtext(0.5, -0.1, "Figure ?: ROC Curve for Logistic Regression Model", ha="center", fontsize=11)
plt.legend()
plt.show()
```

Figure ? shows the ROC graph for the LR model

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_lr = pd.DataFrame(conf_matrix_lr, index=["No Default", "Default"],
                          columns=["No Default", "Default"])

# Plot Confusion Matrix for Logistic Regression
plt.figure(figsize=(8, 4))
sns.heatmap(conf_df_lr, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.figtext(0.5, -0.1, "Figure ?: Confusion Matrix for Logistic Regression Model", ha="center", fontsize=11)
plt.show()
```

Conf Matrix...

```{python}
# Fit logistic regression model using statsmodels for odds ratios
X_train_sm = sm.add_constant(X_train)  # Add constant term for intercept
logit_model = sm.Logit(y_train, X_train_sm).fit()
lr_train = best_lr_model.predict_proba(X_train)[:, 1]  # Extract probability for positive class

# Extract odds ratios and 95% confidence intervals
odds_ratios = np.exp(logit_model.params)
conf = np.exp(logit_model.conf_int())
conf['OR'] = odds_ratios
conf.columns = ['2.5%', '97.5%', 'OR']
conf = conf.reindex((conf['OR'] - 1).abs().sort_values(ascending=True).index)
conf = conf.drop(['const'])

# Plot odds ratios as a forest plot with 95% confidence intervals
plt.figure(figsize=(8, 4))
plt.errorbar(conf['OR'], conf.index, xerr=[conf['OR'] - conf['2.5%'], conf['97.5%'] - conf['OR']], fmt='o', color='black', ecolor='gray', capsize=3)
plt.axvline(x=1, linestyle='--', color='red')
plt.xlabel('Odds Ratio')
plt.ylabel('Features')
plt.figtext(0.5, -0.1, "Figure ?: Odds Ratios for Logistic Regression Model", ha = "center", fontsize=14)
plt.show()
```

Odds ratios were calculated allowing an easy interpretation of the relationships between the factors and credit risk. The odds ratio indicates the increase in the risk of defaulting for a one-unit increase in that variable.

### **3.2 Random Forest**

```{python}
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [2,4],
    'bootstrap': [True, False]
}

# Define Stratified K-Fold
stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Initialize Grid Search with Cross-Validation
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)

# Make Predictions
rf_predictions = best_rf_model.predict(X_test)
rf_probabilities = best_rf_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_rf = confusion_matrix(y_test, rf_predictions)

# Compute Performance Metrics
accuracyRF = round(accuracy_score(y_test, rf_predictions), 3)
precisionRF = round(precision_score(y_test, rf_predictions), 3)
recallRF = round(recall_score(y_test, rf_predictions), 3)
f1_scoreRF = round(f1_score(y_test, rf_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probabilities)
auc_value_RF = round(auc(fpr_rf, tpr_rf), 3)

```

```{python}
# Plot ROC Curve for Random Forest
plt.figure(figsize=(8, 4))
plt.plot(fpr_rf, tpr_rf, color="black", linewidth=2, label=f"AUC: {auc_value_RF}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for Random Forest Model", y = -0.05, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_rf = pd.DataFrame(conf_matrix_rf, index=["No Default", "Default"],
                          columns=["No Default", "Default"])

# Plot Confusion Matrix for Random Forest
plt.figure(figsize=(8, 4))
sns.heatmap(conf_df_rf, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for Random Forest Model", y = -0.05, fontsize=11)
plt.show()
```

```{python}
# Importance values
# Extract feature importances
feature_importances = best_rf_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(8, 4))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: Feature Importances from Random Forest Model", y = -0.05, fontsize=14, x = 0.5)
plt.show()
```

### **3.3 XGBoost**

```{python}
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}

grid_search = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_xgb_model = grid_search.best_estimator_
best_xgb_model.fit(X_train, y_train)

# Make Predictions
xgb_predictions = best_xgb_model.predict(X_test)
xgb_probabilities = best_xgb_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class


# Compute Confusion Matrix
conf_matrix_xgb = confusion_matrix(y_test, xgb_predictions)

# Compute Performance Metrics
accuracyXGB = round(accuracy_score(y_test, xgb_predictions), 3)
precisionXGB = round(precision_score(y_test, xgb_predictions), 3)
recallXGB = round(recall_score(y_test, xgb_predictions), 3)
f1_scoreXGB = round(f1_score(y_test, xgb_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probabilities)
auc_value_XGB = round(auc(fpr_xgb, tpr_xgb), 3)

```

```{python}
# Plot ROC Curve for XGBoost
plt.figure(figsize=(8, 4))
plt.plot(fpr_xgb, tpr_xgb, color="black", linewidth=2, label=f"AUC: {auc_value_XGB}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for XGBoost Model", y = -0.05, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df = pd.DataFrame(conf_matrix_xgb, index=["No Default", "Default"],
                       columns=["No Default", "Default"])

# Plot Confusion Matrix
plt.figure(figsize=(8, 4))
sns.heatmap(conf_df, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for XGBoost Model", y = -0.05, fontsize=11)
plt.show()
```

```{python}
# Extract feature importances
feature_importances = best_xgb_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(8, 4))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: Feature Importances from XGBoost Model", y = -0.05, fontsize=14, x = 0.5)
plt.show()
```

### **3.4 Light Gradient Boosted Machine**

```{python}

# Build the neural network model
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1],
    'num_leaves': [20, 31],
    'subsample': [0.8, 1.0]
}

lgbm_params = {'verbosity': -1}  # Set LightGBM verbosity to -1 (no warnings)

# Initialize Grid Search with Cross-Validation for LGBMClassifier
grid_search = GridSearchCV(LGBMClassifier(**lgbm_params,random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_lgbm_model = grid_search.best_estimator_
best_lgbm_model.fit(X_train, y_train)

# Make Predictions
lgbm_predictions = best_lgbm_model.predict(X_test)
lgbm_probabilities = best_lgbm_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_lgbm = confusion_matrix(y_test, lgbm_predictions)

# Compute Performance Metrics
accuracyLGBM = round(accuracy_score(y_test, lgbm_predictions), 3)
precisionLGBM = round(precision_score(y_test, lgbm_predictions), 3)
recallLGBM = round(recall_score(y_test, lgbm_predictions), 3)
f1_scoreLGBM = round(f1_score(y_test, lgbm_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_lgbm, tpr_lgbm, _ = roc_curve(y_test, lgbm_probabilities)
auc_value_LGBM = round(auc(fpr_lgbm, tpr_lgbm), 3)
```

```{python}

# Plot ROC Curve for LightGBM
plt.figure(figsize=(8, 4))
plt.plot(fpr_lgbm, tpr_lgbm, color="black", linewidth=2, label=f"AUC: {auc_value_LGBM}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for LightGBM Model", y = -0.05, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_lgbm = pd.DataFrame(conf_matrix_lgbm, index=["No Default", "Default"],
                            columns=["No Default", "Default"])

# Plot Confusion Matrix for LightGBM
plt.figure(figsize=(8, 4))
sns.heatmap(conf_df_lgbm, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for LightGBM Model", y = -0.05, fontsize=11)
plt.show()

```

```{python}
# Extract feature importances
feature_importances = best_lgbm_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances in black and white with black outlines
plt.figure(figsize=(8, 4))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: Feature Importances from LightGBM Model", y = -0.05, fontsize=14, x = 0.5)
plt.show()
```

### **3.5 Model Evaluation and Comparisons**

```{python}
# Create Accuracy DataFrame
accuracy_df = pd.DataFrame({
    "Model": ["LR","RF", "XGB", "LGBM"],
    "Value": [accuracyLR,accuracyRF, accuracyXGB, accuracyLGBM]
})

# Create Bar Plot for Accuracy
plt.figure(figsize=(8, 3))
sns.barplot(x="Model", y="Value", data=accuracy_df, palette="gray", edgecolor="black")
for index, row in accuracy_df.iterrows():
    plt.text(index, row.Value + 0.02, round(row.Value, 3), ha="center", fontsize=10)
plt.ylim(0, 1)
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.figtext(0.5, -0.1, "Figure ?: Accuracy for Each Model", ha="center", fontsize=11)
plt.show()

```

```{python}
# Log Loss
log_loss_LR = log_loss(y_test, lr_predictions)
log_loss_RF = log_loss(y_test, rf_predictions)
log_loss_XGB = log_loss(y_test, xgb_predictions)
log_loss_LGBM = log_loss(y_test, lgbm_predictions)

# Brier Scores
brier_LR = brier_score_loss(y_test, lr_predictions)
brier_RF = brier_score_loss(y_test, rf_predictions)
brier_XGB = brier_score_loss(y_test, xgb_predictions)
brier_LGBM = brier_score_loss(y_test, lgbm_predictions)

# Create a DataFrame for Performance Metrics
performance_df = pd.DataFrame({
    "Model": ["LR","RF", "XGB", "LGBM"],
    "Accuracy": [accuracyLR,accuracyRF, accuracyXGB, accuracyLGBM],
    "Precision": [precisionLR,precisionRF, precisionXGB, precisionLGBM],
    "Recall": [recallLR,recallRF, recallXGB, recallLGBM],
    "F1 Score": [f1_scoreLR,f1_scoreRF, f1_scoreXGB, f1_scoreLGBM],
    "AUC": [auc_value_LR,auc_value_RF, auc_value_XGB, auc_value_LGBM],
    "Log Loss": [log_loss_LR,log_loss_RF, log_loss_XGB, log_loss_LGBM],
    "Brier Score": [brier_LR,brier_RF, brier_XGB, brier_LGBM]
})

# Round values for better readability and format as strings for LaTeX output
performance_df = performance_df.round(3).astype(str)

# Convert the DataFrame to LaTeX format with appropriate formatting
performance_latex = performance_df.to_latex(index=False,
                                             caption="Performance Metrics for Each Model",
                                             label="Table 3 :performance_metrics",
                                             column_format="lrrrrrrr",
                                             escape=False)

# Replace underscores with LaTeX-safe versions
performance_latex = performance_latex.replace("AUC", "AUC")
performance_latex = performance_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")

# Save to a LaTeX file
with open("performance_table.tex", "w") as f:
    f.write(performance_latex)
```

\input{performance_table.tex}

## **4. Conclusion**

[Link to Github Repository = https://github.com/JoshLG18/DSE-EMP-Project](https://github.com/JoshLG18/DSE-EMP-Project)