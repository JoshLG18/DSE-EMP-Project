---
title: "Predicting Loan Defaults: A Data-Driven Approach to Credit Risk Analysis"
author: "Student Number - 720017170"
subtitle: BEE2041 - Data Science in Economics
format: pdf
toc: true
execute:
    echo: false
    warning: false
    message: false
    results: false
header-includes:
    - \usepackage{float}  
---

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as mpatches

from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.svm import SVC
import statsmodels.api as sm
from sklearn.utils import resample

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

import warnings
warnings.filterwarnings('ignore')

```

\newpage
## **1. Introduction**


## **2. Data**

```{python}
loan_data = pd.read_csv('Loan_default.csv')
# Data Cleaning
# Drop duplicate rows if any
loan_data.drop_duplicates(inplace=True)

# Handle missing values (if there were any) - To median values
loan_data.dropna()

#Remove LoanID
loan_data.drop(columns=['LoanID'], inplace=True)

# Turn into categorical
loan_data['NumCreditLines'] = loan_data['NumCreditLines'].astype('category')
loan_data['LoanTerm'] = loan_data['LoanTerm'].astype('category')
loan_data['Default'] = loan_data['Default'].astype('category')

# Calculate distribution before downsampling
default_counts_before = loan_data['Default'].value_counts()

# Balance the classes in the Default column
default_0 = loan_data[loan_data['Default'] == 0]
default_1 = loan_data[loan_data['Default'] == 1]

# Downsample majority class
default_0_downsampled = resample(default_0, 
                                 replace=False,    
                                 n_samples=len(default_1),  
                                 random_state=123)

# Combine minority class with downsampled majority class
loan_data_balanced = pd.concat([default_0_downsampled, default_1])

# Calculate distribution after downsampling
default_counts_after = loan_data_balanced['Default'].value_counts()

# Reduce the sample size to 1% of the balanced dataset while maintaining class proportions
loan_data, _ = train_test_split(loan_data_balanced, test_size=0.90, stratify=loan_data_balanced['Default'], random_state=123)


# Create a DataFrame for plotting
distribution_df = pd.DataFrame({
    'Before Downsampling': default_counts_before,
    'After Downsampling': default_counts_after
}).reset_index().melt(id_vars='Default', var_name='Stage', value_name='Count')

```

```{python}
# Create a DataFrame with variable names and data types
variable_info = pd.DataFrame({
    'Variable': loan_data.columns,
    'Data Type': loan_data.dtypes.astype(str)
})

# Add definitions for each variable
variable_info['Definition'] = [
    'Age of the borrower',
    'Income of the borrower',
    'Loan amount requested by the borrower',
    'Credit score of the borrower',
    'Number of months the borrower has been employed',
    'Number of credit lines the borrower has',
    'Interest rate of the loan',
    'Term of the loan in months',
    'Debt-to-Income ratio of the borrower',
    'Education level of the borrower',
    'Employment type of the borrower',
    'Marital status of the borrower',
    'Whether the borrower has a mortgage',
    'Whether the borrower has dependents',
    'Purpose of the loan',
    'Whether the borrower has a co-signer',
    'Whether the borrower defaulted on the loan'
]

# Convert the DataFrame to LaTeX format with appropriate formatting
variable_info_latex = variable_info.to_latex(index=False,
                                             caption="Variable Information",
                                             label="Table 1:variable_info",
                                             column_format="lll",
                                             escape=False)
variable_info_latex = variable_info_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")


# Save to a LaTeX file
with open("variable_info_table.tex", "w") as f:
    f.write(variable_info_latex)
```

\input{variable_info_table.tex}


### **2.1 Descriptive Statistics**
```{python}
# Compute summary statistics
summary_stats = loan_data.describe().transpose()
summary_stats = summary_stats[['count', 'mean', '50%', 'std', 'min', 'max']]
summary_stats.columns = ['N', 'Mean', 'Median', 'SD', 'Min', 'Max']
summary_stats.index.name = "Variable"

# Round values for better readability and format as strings for LaTeX output
summary_stats = summary_stats.round(1).astype(str)

# Convert index to column for better formatting
summary_stats.reset_index(inplace=True)

# Convert table to LaTeX format with formatting
latex_table = summary_stats.to_latex(index=False,
                                     caption="Summary Statistics of Numeric Variables",
                                     label="Table 2:summary_stats",
                                     column_format="lrrrrrr",
                                     escape=False)

latex_table = latex_table.replace("\\begin{table}", "\\begin{table}[H]\\centering")

# Save to a LaTeX file
with open("summary_table.tex", "w") as f:
    f.write(latex_table)
```

\input{summary_table.tex}

### **2.2 Distribution Analysis**

```{python}
numeric_cols = loan_data.select_dtypes(include=[np.number]).columns
num_cols = 4
num_rows = int(np.ceil(len(numeric_cols) / num_cols))

plt.rcParams.update({'font.size': 45})
plt.figure(figsize=(50, 15 * num_rows))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(num_rows, num_cols, i)
    loan_data[col].hist(bins=30, edgecolor='black')
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)  # Tilt the x-axis labels by 45 degrees

plt.tight_layout()
plt.figtext(0.5, -0.01, "Figure ?: Histograms of all Numeric Variables", ha="center", fontsize=55)
plt.show()
plt.rcParams.update({'font.size': 14})

```

```{python}
#Clearning pt 2
# Convert string variables using Label Encoding into categorical
categorical_cols = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    loan_data[col] = le.fit_transform(loan_data[col])
    label_encoders[col] = le
numeric_cols = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']

```

```{python}
loan_data['NumCreditLines'] = loan_data['NumCreditLines'].astype(int)
loan_data['LoanTerm'] = loan_data['LoanTerm'].astype(int)

# Box plots for all numeric variables pre normalisation
plt.rcParams.update({'font.size': 12})
plt.figure(figsize=(12, 5))
loan_data.boxplot()
plt.figtext(0.5, -0.2, "Figure ?: Box Plots of All Variables Before Normalisation", ha="center", fontsize=11)
plt.xticks(rotation=45)
plt.show()
```

```{python}
#Normalisation using z-score normalisation
numeric_cols = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']
scaler = StandardScaler()
loan_data[numeric_cols] = scaler.fit_transform(loan_data[numeric_cols])

# Box plots for all numeric variables post normalisation
plt.figure(figsize=(12,5))
loan_data.boxplot()
plt.figtext(0.5, -0.2, "Figure ?: Box Plots of All Variables After Normalisation", ha="center", fontsize=11)
plt.xticks(rotation=45)
plt.show()
```


```{python}
# Plot the stacked bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Stage', y='Count', hue='Default', data=distribution_df, palette='gray')
plt.xlabel('Stage')
plt.ylabel('Count')
plt.figtext(0.5, -0.01, "Figure ?: Distribution of Default Before and After Downsampling", ha="center", fontsize=11)
plt.legend(title='Default', loc='upper right')
plt.show()
```

### **2.3 Correlation Analysis**

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Correlation plot
plt.figure(figsize=(12, 10))
sns.heatmap(loan_data.corr(method='spearman'), annot=True, cmap='Greys', fmt='.2f', linewidths=0.5)
plt.figtext(0.5, -0.09, "Figure ?: Correlation Plot of All Variables ", ha="center", fontsize=11)
plt.show()

# VIF values
train_data, test_data = train_test_split(loan_data, test_size=0.2, random_state=123)
X_train = train_data.drop(columns=['Default'])

# Add a constant term for intercept
X_train_vif = sm.add_constant(X_train)

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X_train_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_train_vif.values, i) for i in range(X_train_vif.shape[1])]

# Drop the constant term from the VIF DataFrame
vif_data = vif_data[vif_data["Feature"] != "const"]

# Convert the DataFrame to LaTeX format with appropriate formatting
vif_latex = vif_data.to_latex(index=False,
                              caption="Variance Inflation Factor (VIF) Values",
                              label="Table 4:vif_values",
                              column_format="lc",
                              escape=False)
vif_latex = vif_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")
vif_latex = vif_latex.replace("\\end{table}", "\\end{table}")
# Save to a LaTeX file
with open("vif_table.tex", "w") as f:
    f.write(vif_latex)

# Display the VIF DataFrame
```

\input{vif_table.tex}

```{python}
# Feature Selection
# Compute the correlation matrix
corr_matrix = loan_data.corr(method='spearman')

# Filter columns based on correlation with 'Default'
target_corr = corr_matrix['Default']
filtered_cols = target_corr[(target_corr >= 0.05) | (target_corr <= -0.05)].index.tolist()

# Update the dataset to keep only the filtered columns
loan_data = loan_data[filtered_cols]
```


## **3. Results and Discussion**

```{python}
# Split the data into test and train
train_data, test_data = train_test_split(loan_data, test_size=0.2, random_state=123)
# Prepare dataset
X_train = train_data.drop(columns=['Default'])
y_train = train_data['Default']
X_test = test_data.drop(columns=['Default'])
y_test = test_data['Default']
```

### **3.1 Logistic Regression**

```{python}
import sys
import os
sys.stdout = open(os.devnull, 'w')

# Define the parameter grid for Grid Search
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

# Initialize Grid Search with Cross-Validation for Logistic Regression
stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_lr_model = grid_search.best_estimator_
best_lr_model.fit(X_train, y_train)

sys.stdout = sys.__stdout__

# Make Predictions
lr_predictions = best_lr_model.predict(X_test)
lr_probabilities = best_lr_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_lr = confusion_matrix(y_test, lr_predictions)

# Compute Performance Metrics
accuracyLR = round(accuracy_score(y_test, lr_predictions), 3)
precisionLR = round(precision_score(y_test, lr_predictions), 3)
recallLR = round(recall_score(y_test, lr_predictions), 3)
f1_scoreLR = round(f1_score(y_test, lr_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probabilities)
auc_value_LR = round(auc(fpr_lr, tpr_lr), 3)
```

```{python}
# Plot ROC Curve for Logistic Regression
plt.figure(figsize=(8, 5))
plt.plot(fpr_lr, tpr_lr, color="black", linewidth=2, label=f"AUC: {auc_value_LR}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.figtext(0.5, -0.1, "Figure ?: ROC Curve for Logistic Regression Mod", ha="center", fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_lr = pd.DataFrame(conf_matrix_lr, index=["No Default", "Default"],
                          columns=["No Default", "Default"])

# Plot Confusion Matrix for Logistic Regression
plt.figure(figsize=(8, 5))
sns.heatmap(conf_df_lr, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.figtext(0.5, -0.1, "Figure ?: Confusion Matrix for Logistic Regression Model", ha="center", fontsize=11)
plt.show()
```

```{python}
# Fit logistic regression model using statsmodels for odds ratios
X_train_sm = sm.add_constant(X_train)  # Add constant term for intercept
logit_model = sm.Logit(y_train, X_train_sm).fit()
lr_train = best_lr_model.predict_proba(X_train)[:, 1]  # Extract probability for positive class


# Extract odds ratios and 95% confidence intervals
odds_ratios = np.exp(logit_model.params)
conf = np.exp(logit_model.conf_int())
conf['OR'] = odds_ratios
conf.columns = ['2.5%', '97.5%', 'OR']
conf = conf.reindex((conf['OR'] - 1).abs().sort_values(ascending=True).index)
conf = conf.drop(['const'])

# Plot odds ratios as a forest plot with 95% confidence intervals
plt.figure(figsize=(8, 5))
plt.errorbar(conf['OR'], conf.index, xerr=[conf['OR'] - conf['2.5%'], conf['97.5%'] - conf['OR']], fmt='o', color='black', ecolor='gray', capsize=3)
plt.axvline(x=1, linestyle='--', color='red')
plt.xlabel('Odds Ratio')
plt.ylabel('Features')
plt.figtext(0.5, -0.1, "Figure ?: Odds Ratios with 95% Confidence Intervals", ha="center", fontsize=11)
plt.show()
```

### **3.2 Random Forest**

```{python}
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [2,4],
    'bootstrap': [True, False]
}

# Define Stratified K-Fold
stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Initialize Grid Search with Cross-Validation
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)

# Make Predictions
rf_predictions = best_rf_model.predict(X_test)
rf_probabilities = best_rf_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_rf = confusion_matrix(y_test, rf_predictions)

# Compute Performance Metrics
accuracyRF = round(accuracy_score(y_test, rf_predictions), 3)
precisionRF = round(precision_score(y_test, rf_predictions), 3)
recallRF = round(recall_score(y_test, rf_predictions), 3)
f1_scoreRF = round(f1_score(y_test, rf_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probabilities)
auc_value_RF = round(auc(fpr_rf, tpr_rf), 3)

```

```{python}
# Plot ROC Curve for Random Forest
plt.figure(figsize=(8, 5))
plt.plot(fpr_rf, tpr_rf, color="black", linewidth=2, label=f"AUC: {auc_value_RF}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for Random Forest Model", y = 0.0005, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_rf = pd.DataFrame(conf_matrix_rf, index=["No Default", "Default"],
                          columns=["No Default", "Default"])

# Plot Confusion Matrix for Random Forest
plt.figure(figsize=(8, 5))
sns.heatmap(conf_df_rf, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for Random Forest Model", y = 0.0005, fontsize=11)
plt.show()
```

```{python}
# Importance values
# Extract feature importances
feature_importances = best_rf_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(8, 5))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: Feature Importances from Random Forest Model", y = 0.0005, fontsize=11)
plt.show()
```

### **3.3 XGBoost**

```{python}
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}

grid_search = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_xgb_model = grid_search.best_estimator_
best_xgb_model.fit(X_train, y_train)

# Make Predictions
xgb_predictions = best_xgb_model.predict(X_test)
xgb_probabilities = best_xgb_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_xgb = confusion_matrix(y_test, xgb_predictions)

# Compute Performance Metrics
accuracyXGB = round(accuracy_score(y_test, xgb_predictions), 3)
precisionXGB = round(precision_score(y_test, xgb_predictions), 3)
recallXGB = round(recall_score(y_test, xgb_predictions), 3)
f1_scoreXGB = round(f1_score(y_test, xgb_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probabilities)
auc_value_XGB = round(auc(fpr_xgb, tpr_xgb), 3)

```

```{python}
# Plot ROC Curve for XGBoost
plt.figure(figsize=(8, 5))
plt.plot(fpr_xgb, tpr_xgb, color="black", linewidth=2, label=f"AUC: {auc_value_XGB}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for XGBoost Model", y = 0.0005, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df = pd.DataFrame(conf_matrix_xgb, index=["No Default", "Default"],
                       columns=["No Default", "Default"])

# Plot Confusion Matrix
plt.figure(figsize=(8, 5))
sns.heatmap(conf_df, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for XGBoost Model", y = 0.0005, fontsize=11)
plt.show()
```

```{python}
# Extract feature importances
feature_importances = best_xgb_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: eature Importances from XGBoost Model", y = 0.0005, fontsize=11)
plt.show()
```

### **3.4 Light Gradient Boosting Machine (LGBM)**

```{python}
# Define the parameter grid for Grid Search
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1],
    'num_leaves': [20, 31],
    'subsample': [0.8, 1.0]
}

lgbm_params = {'verbosity': -1}  # Set LightGBM verbosity to -1 (no warnings)

# Initialize Grid Search with Cross-Validation for LGBMClassifier
grid_search = GridSearchCV(LGBMClassifier(**lgbm_params,random_state=42), param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_lgbm_model = grid_search.best_estimator_
best_lgbm_model.fit(X_train, y_train)

# Make Predictions
lgbm_predictions = best_lgbm_model.predict(X_test)
lgbm_probabilities = best_lgbm_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_lgbm = confusion_matrix(y_test, lgbm_predictions)

# Compute Performance Metrics
accuracyLGBM = round(accuracy_score(y_test, lgbm_predictions), 3)
precisionLGBM = round(precision_score(y_test, lgbm_predictions), 3)
recallLGBM = round(recall_score(y_test, lgbm_predictions), 3)
f1_scoreLGBM = round(f1_score(y_test, lgbm_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_lgbm, tpr_lgbm, _ = roc_curve(y_test, lgbm_probabilities)
auc_value_LGBM = round(auc(fpr_lgbm, tpr_lgbm), 3)
```

```{python}

# Plot ROC Curve for LightGBM
plt.figure(figsize=(8, 5))
plt.plot(fpr_lgbm, tpr_lgbm, color="black", linewidth=2, label=f"AUC: {auc_value_LGBM}")
plt.plot([0, 1], [0, 1], linestyle="--", color="grey")  # Reference diagonal
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Sensitivity)")
plt.suptitle("Figure ?: ROC Curve for LightGBM Model", y = 0.0005, fontsize=11)
plt.legend()
plt.show()
```

```{python}
# Convert Confusion Matrix to DataFrame for Visualization
conf_df_lgbm = pd.DataFrame(conf_matrix_lgbm, index=["No Default", "Default"],
                            columns=["No Default", "Default"])

# Plot Confusion Matrix for LightGBM
plt.figure(figsize=(8, 5))
sns.heatmap(conf_df_lgbm, annot=True, fmt="d", cmap="Greys", linewidths=0.5, cbar=False, annot_kws={"size": 24})
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.suptitle("Figure ?: Confusion Matrix for LightGBM Model", y = 0.0005, fontsize=11)
plt.show()

```

```{python}
# Extract feature importances
feature_importances = best_lgbm_model.feature_importances_
features = X_train.columns

# Create a DataFrame for plotting
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances in black and white with black outlines
plt.figure(figsize=(8, 5))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette=sns.color_palette("Greys", n_colors=len(importance_df)), edgecolor='black')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.suptitle("Figure ?: eature Importances from LightGBM Model", y = 0.0005, fontsize=11)
plt.show()
```

### **3.5 Model Evaluation and Comparisons**

```{python}
# Create Accuracy DataFrame
accuracy_df = pd.DataFrame({
    "Model": ["LR","RF", "XGB", "LGBM"],
    "Value": [accuracyLR,accuracyRF, accuracyXGB, accuracyLGBM]
})

# Create Bar Plot for Accuracy
plt.figure(figsize=(8, 3))
sns.barplot(x="Model", y="Value", data=accuracy_df, palette="gray", edgecolor="black")
for index, row in accuracy_df.iterrows():
    plt.text(index, row.Value + 0.02, round(row.Value, 3), ha="center", fontsize=10)
plt.ylim(0, 1)
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.figtext(0.5, -0.1, "Figure ?: Accuracy for Each Model", ha="center", fontsize=11)
plt.show()

```

```{python}
# Create a DataFrame for Performance Metrics
performance_df = pd.DataFrame({
    "Model": ["LR","RF", "XGB", "LGBM"],
    "Accuracy": [accuracyLR,accuracyRF, accuracyXGB, accuracyLGBM],
    "Precision": [precisionLR,precisionRF, precisionXGB, precisionLGBM],
    "Recall": [recallLR,recallRF, recallXGB, recallLGBM],
    "F1_Score": [f1_scoreLR,f1_scoreRF, f1_scoreXGB, f1_scoreLGBM],
    "AUC": [auc_value_LR,auc_value_RF, auc_value_XGB, auc_value_LGBM]
})

# Round values for better readability and format as strings for LaTeX output
performance_df = performance_df.round(3).astype(str)

# Convert the DataFrame to LaTeX format with appropriate formatting
performance_latex = performance_df.to_latex(index=False,
                                             caption="Performance Metrics for Each Model",
                                             label="Table 3 :performance_metrics",
                                             column_format="lrrrrrr",
                                             escape=False)

# Replace underscores with LaTeX-safe versions
performance_latex = performance_latex.replace("F1_Score", "F1\\_Score")
performance_latex = performance_latex.replace("AUC", "AUC")
performance_latex = performance_latex.replace("\\begin{table}", "\\begin{table}[H]\\centering")

# Save to a LaTeX file
with open("performance_table.tex", "w") as f:
    f.write(performance_latex)
```

\input{performance_table.tex}

## **4. Conclusion**



Link to Github Repository = https://github.com/JoshLG18/DSE-EMP-Project