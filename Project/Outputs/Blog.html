<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Student Number – 720017170">

<title>Predicting Loan Defaults: A Data-Driven Approach to Credit Risk Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="Blog_files/libs/clipboard/clipboard.min.js"></script>
<script src="Blog_files/libs/quarto-html/quarto.js"></script>
<script src="Blog_files/libs/quarto-html/popper.min.js"></script>
<script src="Blog_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Blog_files/libs/quarto-html/anchor.min.js"></script>
<link href="Blog_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Blog_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Blog_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Blog_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Blog_files/libs/bootstrap/bootstrap-973236bd072d72a04ee9cd82dcc9cb29.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><strong>1. Introduction</strong></a></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data"><strong>2. Data</strong></a>
  <ul class="collapse">
  <li><a href="#preparing-the-data" id="toc-preparing-the-data" class="nav-link" data-scroll-target="#preparing-the-data"><strong>2.1 Preparing the Data</strong></a></li>
  <li><a href="#descriptive-statistics" id="toc-descriptive-statistics" class="nav-link" data-scroll-target="#descriptive-statistics"><strong>2.2 Descriptive Statistics</strong></a></li>
  <li><a href="#distribution-analysis" id="toc-distribution-analysis" class="nav-link" data-scroll-target="#distribution-analysis"><strong>2.3 Distribution Analysis</strong></a></li>
  <li><a href="#correlation-analysis" id="toc-correlation-analysis" class="nav-link" data-scroll-target="#correlation-analysis"><strong>2.4 Correlation Analysis</strong></a></li>
  <li><a href="#causal-analysis" id="toc-causal-analysis" class="nav-link" data-scroll-target="#causal-analysis"><strong>2.5 Causal Analysis</strong></a></li>
  </ul></li>
  <li><a href="#machine-learning-analysis" id="toc-machine-learning-analysis" class="nav-link" data-scroll-target="#machine-learning-analysis"><strong>3. Machine Learning Analysis</strong></a>
  <ul class="collapse">
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><strong>3.1 Logistic Regression</strong></a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest"><strong>3.2 Random Forest</strong></a></li>
  <li><a href="#xgboost" id="toc-xgboost" class="nav-link" data-scroll-target="#xgboost"><strong>3.3 XGBoost</strong></a></li>
  <li><a href="#model-evaluation-and-comparisons" id="toc-model-evaluation-and-comparisons" class="nav-link" data-scroll-target="#model-evaluation-and-comparisons"><strong>3.4 Model Evaluation and Comparisons</strong></a></li>
  <li><a href="#practical-implications-and-limiations" id="toc-practical-implications-and-limiations" class="nav-link" data-scroll-target="#practical-implications-and-limiations"><strong>3.5 Practical Implications and Limiations</strong></a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><strong>4. Conclusion</strong></a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="Blog.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Predicting Loan Defaults: A Data-Driven Approach to Credit Risk Analysis</h1>
<p class="subtitle lead">BEE2041 Data Science in Economics – Empirical Project</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Student Number – 720017170 </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction"><strong>1. Introduction</strong></h2>
<p>Access to credit is a important driver of economic growth, allowing households or businesses to invest, expand and smooth consumption. However, credit risk remains a fundamental challenge for financial institutions, as loan defaulting can lead to substantial financial losses for both the company and stakeholders. The ability to predict these defaults is vital for lending institutions to mitigate their risk and make more informed lending predictions. Recent advancements in machine learning (ML) have aided in the development of robust predictive models that outperform traditional credit-scoring methods (Yang, 2024)</p>
<p>Ensemble methods such as Random Forest (RF), and Extreme Gradient Boosting (XGBoost), have shown significant promise in improving classification accuracy over traditional statistical methods (Yadav, 2025). These models offer enhanced predictive capacity due to their ability to capture non-linear relationships in borrower data, providing financial institutions with more reliable risk assessment (Roy, 2025).</p>
<p>This project aims to explore a data-driven approach to credit risk analysis by using ML methods to predict loan defaulting. Logistic regression (LR), RF, and XGBoost have all been implemented and compared using standard performance metrics such as accuracy, precision, recall, F1-score and area under the curve (AUC). Moreover, exploratory data analysis will be conducted to examine the distribution of important financial variables, identify correlations and allow for optimised feature selection to improve model performance.</p>
<p>Due to the increasing reliance on alternative data sources and advanced computational methods in the financial sector, the results of this project may have significant practical implications. Improved credit risk analysis can help lenders reduce default rates, minimise losses and promote more inclusive access to credit (Ellsworth, 2025). By leveraging the latest ML methods, this project aims to contribute to the growing body of research on predictive analytics in finance and support more robust lending practices.</p>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data"><strong>2. Data</strong></h2>
<p>Prior to conducting the analysis of credit risk, we need to understand and organise the data. For this analysis we will be using a loan defaulting dataset from Kaggle (reference), consisting of 12 variables/columns and 32580 observations, illustrated in Table 1.</p>
<div class="cell" data-execution_count="5">
<div class="cell-output cell-output-display" data-execution_count="5">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table 1 - Variable Information</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Variable</th>
<th data-quarto-table-cell-role="th">Data Type</th>
<th data-quarto-table-cell-role="th">Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PersonAge</td>
<td>int64</td>
<td>Age of the borrower</td>
</tr>
<tr class="even">
<td>PersonIncome</td>
<td>int64</td>
<td>Income of the borrower</td>
</tr>
<tr class="odd">
<td>PersonHomeOwnership</td>
<td>object</td>
<td>Home ownership of the borrower</td>
</tr>
<tr class="even">
<td>PersonEmpLength</td>
<td>float64</td>
<td>Employment length of the borrower</td>
</tr>
<tr class="odd">
<td>LoanIntent</td>
<td>object</td>
<td>Intention of the loan</td>
</tr>
<tr class="even">
<td>LoanGrade</td>
<td>int64</td>
<td>Loan grade</td>
</tr>
<tr class="odd">
<td>LoanAmnt</td>
<td>int64</td>
<td>Amount of the loan (USD)</td>
</tr>
<tr class="even">
<td>LoanIntRate</td>
<td>float64</td>
<td>Loan interest rate</td>
</tr>
<tr class="odd">
<td>LoanStatus</td>
<td>int64</td>
<td>Loan status (0 - not defaulted, 1 - defaulted)</td>
</tr>
<tr class="even">
<td>LoanPercentIncome</td>
<td>float64</td>
<td>Loan percentage of income</td>
</tr>
<tr class="odd">
<td>PreviousDefault</td>
<td>object</td>
<td>If the borrower has defaulted before</td>
</tr>
<tr class="even">
<td>CredHistory</td>
<td>int64</td>
<td>Credit history length</td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="preparing-the-data" class="level3">
<h3 class="anchored" data-anchor-id="preparing-the-data"><strong>2.1 Preparing the Data</strong></h3>
<div class="cell" data-execution_count="6">
<div class="cell-output cell-output-display" data-execution_count="6">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table 2 -Missing Values for Each Variable</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Variable</th>
<th data-quarto-table-cell-role="th">Missing Values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PersonAge</td>
<td>0</td>
</tr>
<tr class="even">
<td>PersonIncome</td>
<td>0</td>
</tr>
<tr class="odd">
<td>PersonHomeOwnership</td>
<td>0</td>
</tr>
<tr class="even">
<td>PersonEmpLength</td>
<td>887</td>
</tr>
<tr class="odd">
<td>LoanIntent</td>
<td>0</td>
</tr>
<tr class="even">
<td>LoanGrade</td>
<td>0</td>
</tr>
<tr class="odd">
<td>LoanAmnt</td>
<td>0</td>
</tr>
<tr class="even">
<td>LoanIntRate</td>
<td>3095</td>
</tr>
<tr class="odd">
<td>LoanStatus</td>
<td>0</td>
</tr>
<tr class="even">
<td>LoanPercentIncome</td>
<td>0</td>
</tr>
<tr class="odd">
<td>PreviousDefault</td>
<td>0</td>
</tr>
<tr class="even">
<td>CredHistory</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Table 2 displays the missing values within the dataset for each variable. The only variables with missing data are <em>PersonEmpLength</em> and <em>LoaanIntRate</em>, containing 887 and 3095 observations with no values, respectively. Missing data can have a large impact on data analysis if not handled properly and can lead to skewed or incorrect conclusions, making handling this data in the correct way crucial. Due to the positively skewed nature of <em>PersonEmpLength</em>, illustrated in Figure 1, median imputation was deployed in order to maintain the observations and not impact sample size. <em>LoanIntRate</em> saw a high correlation with <em>LoanGrade</em>, shown by Figure 5, therefore regression imputation was used to fill these missing variables and not lose sample size. Also, any duplicate observations were removed to mitigate their impact on the models, this reduced the sample size to 32415 observations.</p>
</section>
<section id="descriptive-statistics" class="level3">
<h3 class="anchored" data-anchor-id="descriptive-statistics"><strong>2.2 Descriptive Statistics</strong></h3>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display" data-execution_count="8">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table 3 -Summary Statistics for Each Variable</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Variable</th>
<th data-quarto-table-cell-role="th">N</th>
<th data-quarto-table-cell-role="th">Mean</th>
<th data-quarto-table-cell-role="th">Median</th>
<th data-quarto-table-cell-role="th">SD</th>
<th data-quarto-table-cell-role="th">Min</th>
<th data-quarto-table-cell-role="th">Max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PersonAge</td>
<td>32415.0</td>
<td>27.7</td>
<td>26.0</td>
<td>6.3</td>
<td>20.0</td>
<td>144.0</td>
</tr>
<tr class="even">
<td>PersonIncome</td>
<td>32415.0</td>
<td>65908.6</td>
<td>55000.0</td>
<td>52533.0</td>
<td>4000.0</td>
<td>2039784.0</td>
</tr>
<tr class="odd">
<td>PersonEmpLength</td>
<td>32415.0</td>
<td>4.8</td>
<td>4.0</td>
<td>4.1</td>
<td>0.0</td>
<td>123.0</td>
</tr>
<tr class="even">
<td>LoanGrade</td>
<td>32415.0</td>
<td>1.2</td>
<td>1.0</td>
<td>1.2</td>
<td>0.0</td>
<td>6.0</td>
</tr>
<tr class="odd">
<td>LoanAmnt</td>
<td>32415.0</td>
<td>9594.0</td>
<td>8000.0</td>
<td>6322.8</td>
<td>500.0</td>
<td>35000.0</td>
</tr>
<tr class="even">
<td>LoanIntRate</td>
<td>32415.0</td>
<td>11.0</td>
<td>11.0</td>
<td>3.2</td>
<td>5.4</td>
<td>23.4</td>
</tr>
<tr class="odd">
<td>LoanStatus</td>
<td>32415.0</td>
<td>0.2</td>
<td>0.0</td>
<td>0.4</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>LoanPercentIncome</td>
<td>32415.0</td>
<td>0.2</td>
<td>0.2</td>
<td>0.1</td>
<td>0.0</td>
<td>0.8</td>
</tr>
<tr class="odd">
<td>CredHistory</td>
<td>32415.0</td>
<td>5.8</td>
<td>4.0</td>
<td>4.1</td>
<td>2.0</td>
<td>30.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Table 3 contains the summary statistics for all variables within the dataset. <em>PersonAge</em> and <em>PersonEmpLength</em> show maximum values of 144 and 123 years respectively, which are both above the oldest age a person has lived (122 years), meaning that they are potential errors. To remove these errors from the dataset, both observations where <em>PersonEmpLength</em> was 123 were removed as to not impact the models. For <em>PersonAge</em>, all observations with ages above 144 years were removed. This left <em>PersonAge</em> with a maximum value of 94 and <em>PersonEmpLength</em> with a maximum value of 41, which both are reasonable.</p>
</section>
<section id="distribution-analysis" class="level3">
<h3 class="anchored" data-anchor-id="distribution-analysis"><strong>2.3 Distribution Analysis</strong></h3>
<div id="ae1fbe69" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-10-output-1.png" class="quarto-figure quarto-figure-center figure-img" data-fig-pos="H" width="766" height="788"></p>
</figure>
</div>
</div>
</div>
<p>The histograms shown in Figure 1 illustrate the distributions for each numeric variable. All of the variables shown have positivley skewed distributions. This is due to individuals with low age likely to have low values in each of these variables. <em>PersonAge</em>, <em>PersonEmpLength</em> and <em>CredLength</em> have very similar distributions, indicating potential correlation between these variables.</p>
<div id="99cfb34a" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-12-output-1.png" width="645" height="342" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 2 shows that the data isn’t scaled proportionally, therefore we need to apply a scaling technique. Due to the skewness of all the variables quantile transformation was deployed, normalised data is shown in Figure 3. The plot shows outliers, however there is no reason for these to be errors meaning they will not be removed. For example, the reason for outliers in <em>PersonIncome</em> is due to people earning considerably more than average.</p>
<div id="81e72eb7" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-13-output-1.png" width="644" height="329" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="38b48ae3" class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-15-output-1.png" width="680" height="706" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 4 demonstrates the the distribution of <em>LoanStatus</em> within the dataset. Before downsampling there was a large discrepancy between the number of people who defaulted and who didn’t. This can cause large impacts on the ML models deployed in the analysis, leading to skewed perforamnce metrics as the models will predict the majority class with high accuracy but the minority class with lower accuracy. To circumvent this issue, downsampling was performed to ensure both outcomes had the same number of observations, shown in Figure 4</p>
</section>
<section id="correlation-analysis" class="level3">
<h3 class="anchored" data-anchor-id="correlation-analysis"><strong>2.4 Correlation Analysis</strong></h3>
<div id="c38442a9" class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-17-output-1.png" width="735" height="650" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 5 shows a correlation plot quantifying the relationships between the variables and to the target <em>LoanStatus</em>. <em>LoanGrade</em> and <em>LoanIntRate</em> have a high correlation coefficient (0.96), indicating that they are highly correlated. Also, a similar relationship is shown between <em>PersonAge</em> and <em>CredHistory</em> (r = 0.8). Both these relationships make logical sense as someone who is older who have a longer credit history and as loan grade increases it is likley that the interest rate does as well. Due to the mullticolliearity in the data, these variables may have to be removed however, futher analysis with variance inflation factor (VIF) is required.</p>
<div class="cell" data-execution_count="17">
<div class="cell-output cell-output-display" data-execution_count="17">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table 4 -Variance Inflation Factor (VIF) Values</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Feature</th>
<th data-quarto-table-cell-role="th">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PersonAge</td>
<td>1.501</td>
</tr>
<tr class="even">
<td>PersonIncome</td>
<td>9.368</td>
</tr>
<tr class="odd">
<td>PersonHomeOwnership</td>
<td>1.199</td>
</tr>
<tr class="even">
<td>PersonEmpLength</td>
<td>1.064</td>
</tr>
<tr class="odd">
<td>LoanIntent</td>
<td>1.002</td>
</tr>
<tr class="even">
<td>LoanGrade</td>
<td>2.972</td>
</tr>
<tr class="odd">
<td>LoanAmnt</td>
<td>12.519</td>
</tr>
<tr class="even">
<td>LoanIntRate</td>
<td>3.09</td>
</tr>
<tr class="odd">
<td>LoanPercentIncome</td>
<td>12.02</td>
</tr>
<tr class="even">
<td>PreviousDefault</td>
<td>1.251</td>
</tr>
<tr class="odd">
<td>CredHistory</td>
<td>1.469</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>VIF values for all the variables are shown within Table 4. In contrast to Figure 5, <em>LoanGrade</em>, <em>LoanIntRate</em>, <em>PersonAge</em>, <em>CredHistory</em> have low VIF values, indicating low levels of multicollinearity. However, <em>LoanAmnt</em> and <em>LoanPercentIncome</em> have VIF values greater than 10 which shows multicollinearity and actions need to be taken to ensure they don’t affect the models. For the logistic regression, L1 and L2 regularisation was deployed to reduce the affects of multicollinearity. Other models are tree based and handle multicollinearity well, therefore no futher processing is needed.</p>
<p>Within this analysis, LR, RF, XGboost models will be trained to predict <strong><em>LoanStatus</em></strong> using <em>PersonAge</em>, <em>PersonIncome</em>, <em>PersonHomeOwnership</em>, <em>PersonEmpLength</em>, <em>LoanIntent</em>, <em>LoanGrade</em>, <em>LoanAmnt</em>, <em>LoanIntRate</em>, <em>LoanPercentIncome</em>, <em>PreviousDefault</em> and <em>CredHistory</em>.</p>
</section>
<section id="causal-analysis" class="level3">
<h3 class="anchored" data-anchor-id="causal-analysis"><strong>2.5 Causal Analysis</strong></h3>
<p>This section explores the heterogeneous impact of previous loan default on the likelihood of defaulting again, using a causal forest framework. By estimating Conditional Average Treatment Effects (CATEs), we can uncover how the effect of prior default varies across individual borrower profiles</p>
<div id="3645fa02" class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-19-output-1.png" width="597" height="469" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 6 shows estimated CATEs, revealing a negativley-skewed distribution with most values centered near zero. This suggests that, for the majority of borrowers, previous default status has a limited marginal effect on the likelihood of defaulting again. However, a notable subgroup exhibits significantly positive CATEs, indicating elevated risk. These individuals may represent vulnerable borrower profiles for whom previous financial distress is a strong predictor of future default. The long right tail highlights the importance of heterogeneity in treatment effects and supports the use of a causal forest over average-effect models.</p>
</section>
</section>
<section id="machine-learning-analysis" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-analysis"><strong>3. Machine Learning Analysis</strong></h2>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression"><strong>3.1 Logistic Regression</strong></h3>
<p>The first model deployed was an LR trained on all the standard variables, this model acts as a baseline to compare all more complex models with.</p>
<div id="dd612f35" class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-21-output-1.png" width="752" height="586" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 7 shows the ROC curve for the LR model, an indication of the trade-off between sensitivity and specificity of the model. The model achieved a AUC score of 0.833 which is considered considerable (Çorbacıoğlu, 2023), indicating solid predictive performance when distinguishing between positive outcomes. The model’s curve lies well above the diagonal reference line (AUC = 0.5), which represents random classification, demonstrating its predictive applications. However, the graph shows room for improvement due to true positive rate (TPR) remaining below 0.9.</p>
<div id="0445f919" class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-22-output-1.png" width="731" height="269" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 8 visualises the error within the classification model. The matrix reveals that the model correctly identified 1085 non-default cases (true negatives) and 1092 default cases (true positives), demonstrating its ability to capture both classes effectively. However, 335 non-default cases were misclassified as defaults (false positives), while 324 default cases were incorrectly predicted as non-defaults (false negatives), potentially leading to losses in revenue for a financial institution.</p>
<div id="7acd5259" class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-23-output-1.png" width="740" height="304" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 9 shows the odds ratios for the LR model. Odds ratios were calculated allowing an easy interpretation of the relationships between the individual features and credit risk. The odds ratio indicate the increase in the risk of defaulting for a one-unit increase in that variable. The results indicate that <em>LoanIntRate</em> and <em>LoanPercentIncome</em> have the strongest positive associations with default, with odds ratios of 2.574 and 1.97 , respectively. This suggests that as interest rates or the proportion of income allocated to the loan increase, the likelihood of default rises significantly. Conversely, <em>PersonIncome</em> has an odds ratio of 0.574, implying that higher income levels reduce the probability of default, aligning with expectations in traditional credit risk assessment.</p>
</section>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest"><strong>3.2 Random Forest</strong></h3>
<p>The second model that was developed and compared with the LR model was an RF as they have been shown to have superior performance than LR models (Couronné et al., 2018). This model was trained on all the standard variables.</p>
<div id="63b9bac5" class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-25-output-1.png" width="752" height="567" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The ROC curve, illustrated in Figure 10 for the RF model showcases its improved classification ability in distinguishing between defaulting and non-defaulting cases and can be compared to LRs. The model achieved a excellent AUC of 0.931 (Çorbacıoğlu, 2023), indicating strong predictive capability and shows that more complex models have the potential to improve credit risk prediction, however this highly accurate performance may indicate overfitting.</p>
<div id="7fead8a0" class="cell" data-execution_count="25">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-26-output-1.png" width="731" height="299" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The confusion matrix (Figure 11) for the RF model provides a detailed comparison of actual versus predicted default status. In this case, the model correctly predicted non-default for 1304 instances (True Negatives), and correctly identified defaulting for 1123 instances (True Positives). However, there were 116 false positives, where the model incorrectly predicted defaulting when the actual class was non-default, and 293 false negatives. This confusion matrix reiterates the improved performance from the LR as the incorret classification instances have decreased.</p>
<div id="1c93e05d" class="cell" data-execution_count="26">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-27-output-1.png" width="777" height="314" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 12 demonstrates the most influential features when predicting credit risk by visualising feature importance calculated using mean decrease in accuracy. <em>LoanIntRate</em> is the most important feature suggesting that the proportion of income allocated to a loan has the strongest impact on the model’s predictions, supporting the conclusions from the LR which ranked it second. <em>LoanPercentIncome</em> and <em>PersonIncome</em> are also shown to be within the top 3 most important features as they are in the LR model. Contrastly, to the LR, the RF shows <em>LoanGrade</em> to have high importance whereas Figure 8 shows it to have very little impact on credit risk for the LR model, potentially attributed to the differences in model architecture.</p>
</section>
<section id="xgboost" class="level3">
<h3 class="anchored" data-anchor-id="xgboost"><strong>3.3 XGBoost</strong></h3>
<p>The third model that I deployed to improve upon the RF model was an XGBoost as they have been shown to reduce potential overfitting and have higher performance and speed than RFs (GeeksforGeeks, 2024).</p>
<div id="467d98d1" class="cell" data-execution_count="28">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-29-output-1.png" width="752" height="567" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 13 visualises the performance of the XGBoost in classifying positive outcomes. This model achieves a slightly higher AUC score when compared to the RF (AUC = 0.946), demonstrating excellent predictive performance (Çorbacıoğlu, 2023). Despite the higher AUC score this model is performance is less likely to be attributed to potential overfitting due to built in regularisation parameters; <em>max_depth</em> and <em>min_child_weight</em> along with L1 + L2 regularisation.</p>
<div id="1699e366" class="cell" data-execution_count="29">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-30-output-1.png" width="731" height="266" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 14 reiterates the increased performance of the XGBoost when compared to the RF model. The XGBoost predicts 48 more true positives than the RF model did, indicating better sensitivity which is useful for a lending insitution as this allows them to not give credit to these individuals, avoiding potentially revenue losses. Although the difference is small, defaulting can have large financial losses. The model also has 9 fewer false positives, meaning it incorrectly predicts fewer non-defaulters as defaulters. this figure shows that the XGBoost model demonstrates a balanced improvement, capturing more true positives while keeping false positives low, making it a stronger candidate for credit risk assessment.</p>
<div id="5f055827" class="cell" data-execution_count="30">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-31-output-1.png" width="781" height="267" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 15 contradicts the other models (LR and RF), as these models predicted <em>LoanIntRate</em> to have less of an impact on the preditions than the XGBoost model. However, similar to the RF and LR model, the XGBoost placed high importance on <em>LoanPercentIncome</em> reinforcing the notion that the proportion of income allocated to a loan significantly impacts the risk of defaulting. However, <em>PersonIncome</em> ranks higher than in the RF and LR, indicating that home ownership status may play a larger role in how XGBoosts evaluates credit risk.</p>
</section>
<section id="model-evaluation-and-comparisons" class="level3">
<h3 class="anchored" data-anchor-id="model-evaluation-and-comparisons"><strong>3.4 Model Evaluation and Comparisons</strong></h3>
<div id="4291f077" class="cell" data-execution_count="31">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Blog_files/figure-html/cell-32-output-1.png" width="741" height="310" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="cell-output cell-output-display" data-execution_count="33">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table 5 -Performance Metrics for Each Model</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Model</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
<th data-quarto-table-cell-role="th">Precision</th>
<th data-quarto-table-cell-role="th">Sensitivity</th>
<th data-quarto-table-cell-role="th">F1 Score</th>
<th data-quarto-table-cell-role="th">AUC</th>
<th data-quarto-table-cell-role="th">Log Loss</th>
<th data-quarto-table-cell-role="th">Brier Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LR</td>
<td>0.768</td>
<td>0.765</td>
<td>0.771</td>
<td>0.768</td>
<td>0.833</td>
<td>8.375</td>
<td>0.232</td>
</tr>
<tr class="even">
<td>RF</td>
<td>0.856</td>
<td>0.906</td>
<td>0.793</td>
<td>0.846</td>
<td>0.931</td>
<td>5.198</td>
<td>0.144</td>
</tr>
<tr class="odd">
<td>XGB</td>
<td>0.87</td>
<td>0.904</td>
<td>0.827</td>
<td>0.864</td>
<td>0.946</td>
<td>4.702</td>
<td>0.13</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Figure 16 compares model accuracy, showing that XGBoost (0.87) performs best, followed closely by the RF (0.856), while LR lags behind at 0.768. This highlights the superior performance of ensemble models over LR in classification tasks.</p>
<p>Table 5 provides a more detailed breakdown, confirming XGBoost as the strongest model, with the highest AUC (0.946), recall (0.827), and F1 score (0.864). RF (AUC = 0.931, F1 = 0.846) follow closely, both offering strong predictive performance. LR underperforms across all metrics, with the lowest AUC (0.833) and the highest log loss (8.375), indicating weaker reliability.</p>
<p>The lower log loss and Brier scores for XGBoost reflect better probability calibration, while LR’s higher values suggest less confidence in predictions. Overall, XGBoost provide the best balance of accuracy, precision, and reliability, making them the most effective model in credit risk analysis.</p>
<p>Table 6 summarises the key predictors of default for each model. LR relies on <em>PersonIncome</em>, <em>LoanPercentIncome</em>, and <em>LoanIntRate</em>, suggesting a linear relationship between income, loan percentage, and default risk. RF prioritises <em>LoanPercentIncome</em> and <em>PersonIncome</em>, indicating the importance of relative loan burden, while also recognising <em>LoanIntRate</em>. XGBoost identifies <em>LoanGrade</em> as the most influential feature, followed by <em>PersonHomeOwnership</em> and <em>LoanPercentIncome</em>, reflecting a broader assessment of creditworthiness. The differences suggest that tree-based models (RF, XGBoost) capture non-linear relationships better than LR, which is constrained to linear associations.</p>
</section>
<section id="practical-implications-and-limiations" class="level3">
<h3 class="anchored" data-anchor-id="practical-implications-and-limiations"><strong>3.5 Practical Implications and Limiations</strong></h3>
<p>The findings demonstrate the effectiveness of ensemble learning models, particularly XGBoost and RF, in improving credit risk assessment. These models outperform traditional approaches by capturing complex patterns in financial data and handling imbalanced datasets more effectively (Chopra &amp; Bhilare, 2018). By focusing on key predictors such as <em>LoanGrade</em>, <em>LoanPercentIncome</em>, and <em>PersonIncome</em>, lenders can refine their risk assessment frameworks, improving accuracy while minimising misclassification errors. This enables better-informed lending decisions, reducing default rates and enhancing overall portfolio performance.</p>
<p>Despite their advantages, ensemble models present several challenges. Their complexity reduces interpretability, which is a key consideration in regulated industries where transparency is required (Afolabi, 2024). Ensemble models also require higher computational resources, increasing costs and making them less accessible to smaller institutions (Lei, 2025). Furthermore, ensemble methods are prone to overfitting when applied to imbalanced datasets, limiting their generalisability without proper tuning (Cheng et al., 2021). Another concern is the potential amplification of biases present in training data, which could lead to ethical issues in credit decision-making (Shah &amp; Davis, 2025). Addressing these limitations requires further research into explainable AI, optimised model tuning, and bias mitigation strategies to ensure fair and reliable predictions.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion"><strong>4. Conclusion</strong></h2>
<p>This project compared the effectiveness of LR, RF, and XGB in predicting loan defaults, with results showing that ensemble models significantly outperform traditional approaches. XGB achieved the highest accuracy (0.87) and AUC (0.946), demonstrating its ability to capture complex patterns in financial data. RF also performed well, but its slightly lower recall suggests it may miss some defaulters, leading to financial losses. In contrast, LR lagged behind due to its reliance on linear relationships, making it less suited for this type of predictive task.</p>
<p>Feature importance analysis revealed key differences in how these models assess risk. LR prioritised income-based factors, whereas RF and XGB incorporated broader indicators such as <em>LoanGrade</em> and <em>PersonHomeOwnership</em>. This suggests that ensemble methods offer a more nuanced and holistic approach to credit risk assessment, moving beyond simple financial metrics.</p>
<p>Despite their strengths, XGB and RF come with limitations. Their complexity makes them harder to interpret, which could be a barrier in regulated financial environments. They also require careful tuning to avoid overfitting, meaning their performance depends heavily on parameter selection. Future work should focus on improving explainability, reducing bias in training data, and optimising hyperparameters to further refine predictive performance.</p>
<p><a href="https://github.com/JoshLG18/DSE-EMP-Project">Link to Github Repository = BEE2041 Data Science In Economics Empirical Project</a></p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>