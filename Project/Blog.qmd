---
title: "Predicting Loan Defaults: A Data-Driven Approach to Credit Risk Analysis"
author: "Student Number – 720017170"
subtitle: "BEE2041 Data Science in Economics – Empirical Project"

output-file: Blog  

execute:
  echo: false
  warning: false
  message: false
  results: false
---

:::: {style="display: flex; align-items: center; gap: 1em;"}
::: {style="display: flex; flex-direction: column; align-items: center; margin-right: 1em;"}
<a href="https://github.com/JoshLG18" target="_blank"> <img src="Image/github.png" alt="GitHub" width="50" height="50" style="margin-bottom: 8px;"/> </a> <a href="https://www.linkedin.com/in/josh-le-grice-aa0223261/" target="_blank"> <img src="Image/linkedin.png" alt="LinkedIn" width="50" height="50"/> </a>
:::
::::

![Image from: https://www.istockphoto.com/photos/credit-card-cash](Image/banner.jpg){width="100%" height="200px"}

```{python}
import os
import sys

#Pre-processing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as mpatches
from IPython.display import HTML
from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, brier_score_loss, log_loss, precision_recall_curve
)
from sklearn.preprocessing import label_binarize, LabelEncoder, QuantileTransformer
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
import statsmodels.api as sm
from sklearn.utils import resample
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ML Models
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from xgboost import XGBClassifier
```

\newpage

## **1. Why Predicting Defaults Matters More Than Ever**

Access to credit is a key factor supporting economic development, allowing households and businesses to invest, grow, and manage consumption. However, borrower default remains a constant challenge for financial institutions, often resulting in significant financial losses. Accurate credit risk prediction is essential for reducing exposure and improving lending decisions. Recent advances in machine learning (ML) offer powerful alternatives to traditional scoring models, with ensemble methods such as Random Forest (RF) and Extreme Gradient Boosting (XGBoost) showing strong performance in financial classification tasks (Yang, 2024).

This project compares the performance of Logistic Regression (LR), RF, and XGBoost in predicting loan defaults using five key classification metrics. Accuracy, measuring the overall proportion of correct predictions, while precision evaluates how many predicted defaulters were correctly identified. Recall reflects the proportion of actual defaulters detected by the model. F1-score is the mean of precision and recall, providing a balance between the two, and AUC (Area Under the ROC Curve) assesses the model's ability to rank defaulters above non-defaulters (Saito & Rehmsmeier, 2015).

Given the financial sector’s growing need for alternative data and algorithmic decision-making, this work has practical relevance. Enhanced risk models can reduce default rates, minimise financial losses, and support more inclusive access to credit. The project aims to contribute to ongoing research in predictive analytics and the development of robust, data-driven lending practices.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

## **2. Exploring Risk: From Patterns to Causal Effects**

```{python}
#Loading the data
loan_data = pd.read_csv('Data/credit_risk_dataset.csv')
nrows_pre = loan_data.shape[0]
ncol_pre = loan_data.shape[1]
loan_data.columns = ['PersonAge', 'PersonIncome', 'PersonHomeOwnership', 'PersonEmpLength', 'LoanIntent', 'LoanGrade', 'LoanAmnt', 'LoanIntRate', 'LoanStatus', 'LoanPercentIncome', 'PreviousDefault', 'CredHistory']
```

Prior to conducting the analysis of credit risk, we need to understand and organise the data. For this analysis we will be using a loan defaulting dataset from Kaggle (reference), consisting of `{python} str(ncol_pre)` variables/columns and `{python} str(nrows_pre)` observations, illustrated in Table 1.

```{python}
# Data Cleaning

# Drop duplicate rows if any
loan_data.drop_duplicates(inplace=True)

nrow_post = loan_data.shape[0]

# Create a DataFrame with variable names, data types and missings

# VARIABLE DEFINITIONS + MISSING VALUES
variable_info = pd.DataFrame({
    'Variable': loan_data.columns,
    'Data Type': loan_data.dtypes.astype(str),
    'Definition': [
        'Age of the borrower',
        'Income of the borrower',
        'Home ownership of the borrower',
        'Employment length of the borrower',
        'Intention of the loan',
        'Loan grade',
        'Amount of the loan (USD)',
        'Loan interest rate',
        'Loan status (0 – not defaulted, 1 – defaulted)',
        'Loan percentage of income',
        'If the borrower has defaulted before',
        'Credit history length'
    ]
})

# Count missing values
missing_values = loan_data.isnull().sum().reset_index()
missing_values.columns = ['Variable', 'Missing Values']
variable_info = variable_info.merge(missing_values, on='Variable', how='left')
variable_info['Missing Values'] = variable_info['Missing Values'].fillna(0).astype(int)

```

```{python}
# Sorting out missings and label encoding

# Replacing emplength with median due to skew
loan_data["PersonEmpLength"].fillna(loan_data["PersonEmpLength"].median(), inplace=True)


# Regression imputation for loanintrate as corr with loangrade

    # Convert LoanGrade to a categorical variable
loan_data['LoanGrade'] = loan_data['LoanGrade'].astype('category')

    # Label encode the LoanGrade column
label_encoder = LabelEncoder()
original_label_encoder = label_encoder

loan_data['LoanGrade'] = label_encoder.fit_transform(loan_data['LoanGrade'])

    # Separate train (non-missing LoanIntRate) and test (missing LoanIntRate)
train_data = loan_data.dropna(subset=["LoanIntRate"])
test_data = loan_data[loan_data["LoanIntRate"].isna()]  # Data where LoanIntRate is missing

    # Train a Linear Regression model to predict LoanIntRate
model = LinearRegression()
model.fit(train_data[["LoanGrade"]], train_data["LoanIntRate"])

    # Predict missing LoanIntRate values using LoanGrade
loan_data.loc[loan_data["LoanIntRate"].isna(), "LoanIntRate"] = model.predict(test_data[["LoanGrade"]])


# Removing unrealistic numbers
max_PA = loan_data['PersonAge'].max()
max_PEL = loan_data['PersonEmpLength'].max().astype(int)
loan_data = loan_data[loan_data['PersonAge'] < 123] 
loan_data = loan_data[loan_data['PersonEmpLength'] < 123]
max_PA_after = loan_data['PersonAge'].max()
max_PEL_after = loan_data['PersonEmpLength'].max().astype(int)

# Creating variable for use in text
missing_PersonEmpLength = variable_info.loc[variable_info['Variable'] == 'PersonEmpLength', 'Missing Values'].values[0]
missing_LoanIntRate = variable_info.loc[variable_info['Variable'] == 'LoanIntRate', 'Missing Values'].values[0]
```

```{python}
categorical_vars = ['PersonHomeOwnership', 'LoanIntent', 'LoanGrade', 'LoanStatus', 'PreviousDefault']

# DESCRIPTIVE STATISTICS for numeric only
summary_stats = loan_data.drop(columns=categorical_vars).describe().transpose()
summary_stats = summary_stats[['count', 'mean', '50%', 'std', 'min', 'max']]
summary_stats.columns = ['N', 'Mean', 'Median', 'SD', 'Min', 'Max']
summary_stats = summary_stats.reset_index().rename(columns={'index': 'Variable'})
summary_stats['N'] = summary_stats['N'].round(0).astype(int)
summary_stats[['Mean', 'Median', 'SD', 'Min', 'Max']] = summary_stats[['Mean', 'Median', 'SD', 'Min', 'Max']].round(1)

nrow = loan_data.shape[0]
# Add dummy rows for categorical variables
cat_rows = pd.DataFrame({
    'Variable': categorical_vars,
    'N': nrow,
    'Mean': 'Categorical variable',
    'Median': '',
    'SD': '',
    'Min': '',
    'Max': ''
})

# Combine numeric + dummy categorical summary
summary_stats = pd.concat([summary_stats, cat_rows], ignore_index=True)

# MERGE BOTH TOGETHER
combined_df = variable_info.merge(summary_stats, on='Variable', how='left')

# REORDER COLUMNS
combined_df = combined_df[['Variable', 'Data Type', 'Definition', 'Missing Values', 'N', 'Mean', 'Median', 'SD', 'Min', 'Max']]
```

### **2.1 Preparing the Data and Descriptive Statistics**

```{python}
#| tbl-cap: "Table 1: Variable Information"
HTML(combined_df.to_html(index=False))
```

Table 1 displays missing values and summary statistics for all variables. Only *PersonEmpLength* and *LoanIntRate* had missing values, with `{python} str(missing_PersonEmpLength)` and `{python} str(missing_LoanIntRate)`, respectively. To maintain sample size and address skew, median imputation was used for *PersonEmpLength*, while *LoanIntRate*, highly correlated with *LoanGrade* (see Figure 5), was imputed using regression. Duplicate rows were also removed, reducing the dataset to `{python} str(nrow_post)` observations. Additionally, implausible maximum values for *PersonAge* and *PersonEmpLength* (`{python} str(max_PA)` and `{python} str(max_PEL)`) were treated as errors and removed. After filtering, the adjusted maximum values were `{python} str(max_PA_after)` for *PersonAge* and `{python} str(max_PEL_after)` for *PersonEmpLength*, ensuring data integrity.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

### **2.2 Distribution Analysis**

```{python}
#| fig-align: "center"
#| fig-pos: "H"
#| label: fig-dist
#| fig-cap: "Histograms of all Numeric Variables"

# Set Loan Status to Categorical

loan_data['LoanStatus'] = loan_data['LoanStatus'].astype('category')

numeric_cols = loan_data.select_dtypes(include=[np.number]).columns
numeric_cols = numeric_cols.drop(['LoanGrade'])
num_cols = 2
num_rows = int(np.ceil(len(numeric_cols) / num_cols))

plt.rcParams.update({'font.size': 8})
plt.figure(figsize=(8, 2 * num_rows))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(num_rows, num_cols, i)
    loan_data[col].hist(bins=30, edgecolor='black')
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)  # Tilt the x-axis labels by 45 degrees

plt.tight_layout()
plt.show()
```

The histograms shown in Figure 1 illustrate the distributions for each numeric variable. All of the variables shown have positively skewed distributions. This is due to individuals with low age likely to have low values in each of these variables. *PersonAge*, *PersonEmpLength* and *CredHistory* have very similar distributions, indicating a potential correlation between these variables.

```{python}
#| label: fig-data-over
#| fig-cap: "Data Preparation Overview"

# Save original data for pre-normalisation plot
loan_data_pre = loan_data.copy()

# === PREPARE ===
categorical_cols = ['PersonHomeOwnership', 'LoanIntent', 'PreviousDefault']
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    loan_data[col] = le.fit_transform(loan_data[col])
    label_encoders[col] = le

causal_data = loan_data.copy()

# === TRANSFORMATION ===
numeric_cols = ['PersonAge', 'PersonIncome', 'PersonHomeOwnership', 'PersonEmpLength',
                'LoanIntent', 'LoanGrade', 'LoanAmnt', 'LoanIntRate',
                'LoanPercentIncome', 'PreviousDefault', 'CredHistory']

scaler = QuantileTransformer(output_distribution='normal')
loan_data[numeric_cols] = pd.DataFrame(
    scaler.fit_transform(loan_data[numeric_cols]),
    columns=numeric_cols, index=loan_data.index
)

# Plotting class imbalance
default_counts = loan_data['LoanStatus'].value_counts()
distribution_df = pd.DataFrame({
    'LoanStatus': default_counts.index,
    'Count': default_counts.values
})

distribution_df['Stage'] = 'Loan Status'

# === COMBINED PLOT ===
fig, axes = plt.subplots(3, 1, figsize=(8, 8))

# Boxplot before normalisation
loan_data_pre.boxplot(ax=axes[0])
axes[0].set_title("(a): Box Plots of ALL Variables Before Normalisation", fontsize=10)
axes[0].tick_params(axis='x', labelrotation=45)

# Boxplot after normalisation
loan_data.boxplot(ax=axes[1])
axes[1].set_title("(b): Box Plots of ALL Variables After Normalisation", fontsize=10)
axes[1].tick_params(axis='x', labelrotation=45)

# Class imbalance plot (no downsampling)
sns.barplot(x='Stage', y='Count', hue='LoanStatus', data=distribution_df,
            palette=['#4A90E2', '#003366'], ax=axes[2])
axes[2].set_title("(c): Distribution of LoanStatus", fontsize=10)
axes[2].set_ylabel("Count")
axes[2].set_xlabel("")  # removes the 'Stage' label
axes[2].legend(title='Default', loc='upper right')

plt.tight_layout(rect=[0, 0, 1, 0.98])
plt.show()

```

Figure 2(a) shows that the data isn't scaled proportionally, therefore we need to apply a scaling technique. Due to the skewness of all the variables quantile transformation was deployed, normalised data is shown in Figure 2 (b). The plot shows outliers, however there is no reason for these to be errors meaning they will not be removed. For example, the reason for outliers in *PersonIncome* is due to people earning considerably more than average.

Figure 2(c) demonstrates the distribution of *LoanStatus* within the dataset. This can cause large impacts on the ML models deployed in the analysis, leading to skewed performance metrics as the models will predict the majority class with high accuracy but the minority class with lower accuracy. To circumvent this issue, I implemented class weighting within my ML models.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

### **2.3 Correlation and Multicollinearity Analysis**

```{python}
plt.rcParams.update({'font.size': 10})

# Split the data into test and train
train_data, test_data = train_test_split(loan_data, test_size=0.2, random_state=123)
# Prepare dataset
X_train = train_data.drop(columns=['LoanStatus'])
y_train = train_data['LoanStatus']
X_test = test_data.drop(columns=['LoanStatus'])
y_test = test_data['LoanStatus']
```

```{python}
#| label: fig-Corr-Multi
#| fig-cap: "Correlation and Multicollinearity Analysis"

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Compute correlation
corr_matrix = loan_data.corr(method='spearman')

# Compute VIF
X_vif = sm.add_constant(X_train)
vif_data = pd.DataFrame()
vif_data["Feature"] = X_train.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i+1) for i in range(len(X_train.columns))]
vif_data = vif_data.round(2).sort_values(by="VIF", ascending=False)

# Plot layout: 2 rows — heatmap on top, VIF table below
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12), gridspec_kw={'height_ratios': [2, 1]})

# Correlation matrix
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="Blues", linewidths=0.5, ax=ax1)
ax1.set_title("(a): Correlation Matrix (Spearman)")

# VIF table
ax2.axis('off')
table = ax2.table(cellText=vif_data.values,
                  colLabels=vif_data.columns,
                  cellLoc='center',
                  loc='upper left')

# Make header bold
for (row, col), cell in table.get_celld().items():
    if row == 0:  # Header row
        cell.set_text_props(weight='bold')

table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(0.7, 2)
ax2.set_title("(b): Variance Inflation Factors (VIF)",pad = 5, x = 0.37)

plt.tight_layout()
plt.show()

LG_vs_LIR = corr_matrix.loc["LoanIntRate", "LoanGrade"].round(2)
PA_vs_CH = corr_matrix.loc["CredHistory", "PersonAge"].round(2)
```

Figure 3(a) shows a correlation plot quantifying the relationships between the variables and the target *LoanStatus*. *LoanGrade* and *LoanIntRate* have a high correlation coefficient (r = `{python} str(LG_vs_LIR)`), indicating that they are highly correlated. Also, a similar relationship is shown between *PersonAge* and *CredHistory* (r = `{python} str(PA_vs_CH)`). These patterns are expected—older borrowers tend to have longer credit histories, and higher loan grades are typically associated with higher interest rates. While such correlations suggest potential multicollinearity, further assessment using variance inflation factors (VIF) is required.

VIF values for all the variables are shown in Figure 3(b). In contrast to Figure 3(a), *LoanGrade*, *LoanIntRate*, *PersonAge*, and *CredHistory* have low VIF values, indicating low levels of multicollinearity. However, *LoanAmnt*, *LoanPercentIncome* and *PersonIncome* have VIF values greater than 10 which indicates multicollinearity and actions need to be taken to ensure they don't affect the models. For the logistic regression, L1 and L2 regularisation was deployed to reduce the effects of multicollinearity. Other models are tree-based and handle multicollinearity, therefore no further processing is needed.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

### **2.4 Causal Analysis**

This section explores the heterogeneous impact of *PreviousDefault* on the likelihood of defaulting again, using a causal forest framework. *PreviousDefault* was used within this analysis as Hand & Henley (1997) observed past defaulting status has a strong association with future loan defaults. By estimating Conditional Average Treatment Effects (CATEs), we can observe how the effect of previous default varies across individual borrower profiles.

```{python}
#| label: fig-causal_a
#| fig-cap: "Estimated Treatment Effects (CATE) for Previous Default"

#Have to simulate a causal forest as econml won't install

# Load your dataset
# Assumes LoanStatus is the binary outcome
# and PreviousDefault is the binary treatment (0/1)
X = causal_data.drop(columns=['LoanStatus', 'PreviousDefault'])
y = causal_data['LoanStatus']
t = causal_data['PreviousDefault']

# Split treated and control groups
X_treated = X[t == 1]
y_treated = y[t == 1]

X_control = X[t == 0]
y_control = y[t == 0]

# Train separate models
model_t = RandomForestRegressor(n_estimators=100, random_state=42)
model_t.fit(X_treated, y_treated)

model_c = RandomForestRegressor(n_estimators=100, random_state=42)
model_c.fit(X_control, y_control)

# Predict potential outcomes for all individuals
mu1 = model_t.predict(X)  # potential outcome if treated
mu0 = model_c.predict(X)  # potential outcome if not treated

# Estimate individual treatment effect (ITE)
causal_data['CATE_estimate'] = mu1 - mu0

# Plot the distribution of estimated treatment effects
plt.figure(figsize=(7, 5))
sns.histplot(causal_data['CATE_estimate'], bins=30, kde=True)
plt.xlabel('Estimated CATE')
plt.ylabel('Frequency')
plt.show()
```

Figure 4 shows estimated CATEs, revealing a negatively-skewed distribution with most values centred near zero. This suggests that, for the majority of borrowers, previous default status has a marginal effect on the likelihood of defaulting again. However, a distinct subgroup exhibits significantly higher CATEs, indicating a substantially increased default risk following a previous default. These individuals may represent vulnerable borrowers, for whom financial distress is a strong predictor of future behaviour. The long right tail emphasizes the importance of heterogeneity in treatment effects and justifies the use of a causal forest over average-effect models.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

## **3. Can Machines Predict Who Defaults?**

Within this analysis, LR, RF, and XGboost models will be trained to predict ***LoanStatus*** using *PersonAge*, *PersonIncome*, *PersonHomeOwnership*, *PersonEmpLength*, *LoanIntent*, *LoanGrade*, *LoanAmnt*, *LoanIntRate*, *LoanPercentIncome*, *PreviousDefault* and *CredHistory*.

Before training the models, the dataset was split into train and test sets using an 80:20 ratio to ensure fair evaluation on unseen data. To optimise model performance and avoid overfitting, hyperparameter tuning was conducted using grid search combined with 3-fold cross-validation and L1 + L2 regularisation. To deal with the class imbalance, class weighting was implemented along with prioritising F1-score to reduce financial losses from false negatives but allow the model to remain precise.

### **3.1 Logistic Regression**

The first model deployed was an LR trained on all the standard variables, this model acts as a baseline to compare all more complex models with.

```{python}
sys.stdout = open(os.devnull, 'w')

# Define the parameter grid for Grid Search
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

# Initialize Grid Search with Cross-Validation for Logistic Regression
stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(
    LogisticRegression(class_weight='balanced', random_state=42),
    param_grid,
    cv=stratified_kfold,
    scoring='f1',
    verbose=0,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_lr_model = grid_search.best_estimator_
best_lr_model.fit(X_train, y_train)

sys.stdout = sys.__stdout__

# Make Predictions
lr_predictions = best_lr_model.predict(X_test)
lr_probabilities = best_lr_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_lr = confusion_matrix(y_test, lr_predictions)

# Compute Performance Metrics
accuracyLR = round(accuracy_score(y_test, lr_predictions), 3)
precisionLR = round(precision_score(y_test, lr_predictions), 3)
recallLR = round(recall_score(y_test, lr_predictions), 3)
f1_scoreLR = round(f1_score(y_test, lr_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probabilities)
auc_value_LR = round(auc(fpr_lr, tpr_lr), 3)

```

```{python}
#| label: fig-LR-Vis
#| fig-cap: "Logistic Regression Model Visualizations"

# --- Fit statsmodels logit for odds ratios ---
X_train_sm = sm.add_constant(X_train)
logit_model = sm.Logit(y_train, X_train_sm).fit(disp=False)

# Predict probabilities
lr_train = best_lr_model.predict_proba(X_train)[:, 1]

# Compute ROC curve and AUC
fpr, tpr, _ = roc_curve(y_train, lr_train)
roc_auc = auc(fpr, tpr)

# Convert Confusion Matrix to DataFrame
conf_df_lr = pd.DataFrame(conf_matrix_lr, index=["No Default", "Default"],
                          columns=["No Default", "Default"])

# Extract ORs and confidence intervals
odds_ratios = np.exp(logit_model.params)
conf = np.exp(logit_model.conf_int())
conf['OR'] = odds_ratios
conf.columns = ['2.5%', '97.5%', 'OR']
conf = conf.drop(['const'])
conf = conf.reindex((conf['OR'] - 1).abs().sort_values(ascending=True).index)

# --- Create Grid Layout: ROC on top, CM and OR below ---
fig = plt.figure(constrained_layout=True, figsize=(10, 10))
gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1])

# Top: ROC Curve
ax0 = fig.add_subplot(gs[0, :])
ax0.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')
ax0.plot([0, 1], [0, 1], linestyle='--', color='red')
ax0.set_title('(a): ROC Curve')
ax0.set_xlabel('False Positive Rate')
ax0.set_ylabel('True Positive Rate')
ax0.legend(loc='lower right')

# Bottom Left: Confusion Matrix
ax1 = fig.add_subplot(gs[1, 0])
sns.heatmap(conf_df_lr, annot=True, fmt="d", cmap="Blues",
            linewidths=0.5, cbar=False, annot_kws={"size": 18}, ax=ax1)
ax1.set_title("(b): Confusion Matrix")
ax1.set_xlabel("Predicted Class")
ax1.set_ylabel("Actual Class")

# Bottom Right: Odds Ratios (Forest Plot)
ax2 = fig.add_subplot(gs[1, 1])
ax2.errorbar(conf['OR'], conf.index,
             xerr=[conf['OR'] - conf['2.5%'], conf['97.5%'] - conf['OR']],
             fmt='o', color='black', ecolor='gray', capsize=3)
ax2.axvline(x=1, linestyle='--', color='red')
ax2.set_title('(c): Odds Ratios')
ax2.set_xlabel('Odds Ratio')
ax2.set_ylabel('Features')

plt.tight_layout(rect=[0, 0, 1, 1])  # Reduce top padding
plt.show()

TN = conf_df_lr.iloc[0, 0]  # True Negatives (No Default correctly classified)
FP = conf_df_lr.iloc[0, 1]  # False Positives (Non-default misclassified as Default)
FN = conf_df_lr.iloc[1, 0]  # False Negatives (Default misclassified as Non-default)
TP = conf_df_lr.iloc[1, 1]  # True Positives (Default correctly classified)

top_features_lr = conf.index[-3:].tolist()  # Logistic Regression (based on Odds Ratios)
Feature_1 = top_features_lr[2]
Feature_2 = top_features_lr[1]
Feature_3 = top_features_lr[0]

odds_Feature_1 = odds_ratios[Feature_1].round(3)
odds_Feature_2 = odds_ratios[Feature_2].round(3)
odds_Feature_3 = odds_ratios[Feature_3].round(3)
```

Figure 5(a) shows the ROC curve for the LR model, an indication of the trade-off between sensitivity and specificity of the model. The model achieved an AUC score of `{python} str(auc_value_LR)`, regarded as considerable (Çorbacıoğlu, 2023), indicating solid predictive performance when distinguishing between positive outcomes. The model's curve lies well above the diagonal reference line (AUC = 0.5), which represents random classification, demonstrating its predictive applications. However, the graph shows room for improvement due to the true positive rate (TPR) remaining below 0.9.

Figure 5(b) visualises the error within the classification model. The matrix reveals that the model correctly identified `{python} str(TN)` non-default cases (true negatives) and `{python} str(TP)` default cases (true positives), demonstrating its ability to capture both classes effectively. However, `{python} str(FP)` non-default cases were misclassified as defaults (false positives), while `{python} str(FN)` defaulters were missed (false negatives), which could result in financial loss for lenders.

Figure 5(c) shows the odds ratios for the LR model. The odds ratio indicates the increase in the risk of defaulting for a one-unit increase in that variable and allows for an easy interpretation of the relationships between the individual features and credit risk. The results indicate that *`{python} str(Feature_1)`* and *`{python} str(Feature_2)`* have the strongest positive associations with default, with odds ratios of `{python} str(odds_Feature_1)` and `{python} str(odds_Feature_2)`, respectively. This suggests that as interest rates or the proportion of income allocated to the loan increases, the likelihood of default rises significantly. Conversely, *`{python} str(Feature_3)`* has an odds ratio of `{python} str(odds_Feature_3)`, indicates that higher income lowers default probability—consistent with economic expectations.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

### **3.2 Random Forest**

The second model that was developed and compared with the LR model was an RF, trained on all the standard variables, as they have been shown to have superior performance than LR models (Couronné et al., 2018).

```{python}
# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [100],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5,10],
    'min_samples_leaf': [2,4],
    'bootstrap': [False]
}

# Define Stratified K-Fold
stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Initialize Grid Search with Cross-Validation
grid_search = GridSearchCV(
    RandomForestClassifier(class_weight='balanced', random_state=42),
    param_grid,
    cv=stratified_kfold,
    scoring='f1',     # Focused on minimizing false negatives
    verbose=0,
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)

# Make Predictions
rf_predictions = best_rf_model.predict(X_test)
rf_probabilities = best_rf_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_rf = confusion_matrix(y_test, rf_predictions)

# Compute Performance Metrics
accuracyRF = round(accuracy_score(y_test, rf_predictions), 3)
precisionRF = round(precision_score(y_test, rf_predictions), 3)
recallRF = round(recall_score(y_test, rf_predictions), 3)
f1_scoreRF = round(f1_score(y_test, rf_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probabilities)
auc_value_RF = round(auc(fpr_rf, tpr_rf), 3)

```

```{python}
#| label: fig-RF-Vis
#| fig-cap: "Random Forest Model Visualizations"

# --- Random Forest: Feature Importances ---
feature_importances = best_rf_model.feature_importances_
features = X_train.columns
importance_df_rf = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

conf_df_rf = pd.DataFrame(conf_matrix_rf, index=["No Default", "Default"],
                          columns=["No Default", "Default"])

# --- Create Grid Layout ---
fig = plt.figure(constrained_layout=True, figsize=(10, 10))
gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1])

# Row 1: ROC Curve (Random Forest)
ax0 = fig.add_subplot(gs[0, :])
ax0.plot(fpr_rf, tpr_rf, color="blue", linewidth=2, label=f"AUC: {auc_value_RF:.2f}")
ax0.plot([0, 1], [0, 1], linestyle="--", color="red")
ax0.set_xlabel("False Positive Rate (1 - Specificity)")
ax0.set_ylabel("True Positive Rate (Sensitivity)")
ax0.set_title("(a): ROC Curve ")
ax0.legend()

# Row 2 Col 1: Confusion Matrix (Random Forest)
ax1 = fig.add_subplot(gs[1, 0])
sns.heatmap(conf_df_rf, annot=True, fmt="d", cmap="Blues", linewidths=0.5, cbar=False, annot_kws={"size": 18}, ax=ax1)
ax1.set_xlabel("Predicted Class")
ax1.set_ylabel("Actual Class")
ax1.set_title("(b): Confusion Matrix")

# Row 2 Col 2: Feature Importances (Random Forest)
ax2 = fig.add_subplot(gs[1, 1])
sns.barplot(x='Importance', y='Feature', data=importance_df_rf,
            palette=sns.color_palette("Blues", n_colors=len(importance_df_rf)),
            edgecolor='black', ax=ax2)
ax2.set_xlabel("Importance")
ax2.set_ylabel("Feature")
ax2.set_title("(c): Feature Importances")

plt.tight_layout(rect=[0, 0, 1, 0.96])  # Reduce top padding
plt.show()

TN_rf = conf_df_rf.iloc[0, 0]  # True Negatives (No Default correctly classified)
FP_rf = conf_df_rf.iloc[0, 1]  # False Positives (Non-default misclassified as Default)
FN_rf = conf_df_rf.iloc[1, 0]  # False Negatives (Default misclassified as Non-default)
TP_rf = conf_df_rf.iloc[1, 1]  # True Positives (Default correctly classified)

#Extract most important features
top_features_rf = importance_df_rf.nlargest(3, 'Importance')['Feature'].tolist()  # Random Forest
Feature_1 = top_features_lr[2]
Feature_2 = top_features_lr[1]
Feature_3 = top_features_lr[0]
Feature_4 = importance_df_rf.iloc[3,0]

imp_Feature_1 = importance_df_rf.iloc[0,1].round(3)
imp_Feature_2 = importance_df_rf.iloc[1,1].round(3)
imp_Feature_3 = importance_df_rf.iloc[2,1].round(3)
```

The ROC curve, illustrated in Figure 6(a), showcases its improved classification ability in distinguishing between cases and can be compared to LRs. The model achieved a excellent AUC of `{python} str(auc_value_RF)` (Çorbacıoğlu, 2023), indicating strong predictive capability and shows that more complex models have the potential to improve credit risk prediction, however highly accurate performance may indicate overfitting.

The confusion matrix (Figure 6(b)) provides a detailed comparison of actual versus predicted default status. In this case, the model correctly predicted non-default for `{python} str(TN_rf)` instances (True Negatives), and correctly identified defaulting for `{python} str(TP_rf)` instances (True Positives). However, there were `{python} str(FP_rf)` false positives, where the model incorrectly predicted defaulting when the actual class was non-default, and `{python} str(FN_rf)` false negatives. This confusion matrix reiterates the improved performance from the LR as the incorrect classification instances have decreased.

Figure 6(c) demonstrates the most influential features when predicting credit risk by visualising feature importance calculated using mean decrease in accuracy. *`{python} Feature_1`* is the most important feature suggesting that the proportion of income allocated to a loan has the strongest impact on the model’s predictions, supporting the conclusions from the LR which ranked it second. *`{python} Feature_2`* and *`{python} Feature_3`* are also shown to be within the top 3 most important features as they are in the LR model. Conversely, to the LR, the RF shows *`{python} Feature_4`* to have high importance whereas Figure 8 shows it to have very little impact on credit risk for the LR model, potentially attributed to the differences in model architecture.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

### **3.3 XGBoost**

The third model that I deployed to improve upon the RF model was an XGBoost as they have been shown to reduce potential overfitting and have higher performance and speed than RFs (GeeksforGeeks, 2024).

```{python}
# Define the parameter grid for Grid Search
# Calculate the class imbalance ratio
neg, pos = np.bincount(y_train)
scale_pos_weight = neg / pos

# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [200],
    'max_depth': [10, 20],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'min_child_weight': [1, 5],
    'reg_alpha': [0, 0.1],
    'reg_lambda': [0.5, 1],
    'scale_pos_weight': [scale_pos_weight],  # 👈 Add this
}

# Initialize Grid Search with f1 scoring
grid_search = GridSearchCV(
    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='aucpr'),
    param_grid,
    cv=stratified_kfold,
    scoring='f1',
    verbose=0,
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the best model
best_xgb_model = grid_search.best_estimator_
best_xgb_model.fit(X_train, y_train)

# Make Predictions
xgb_predictions = best_xgb_model.predict(X_test)
xgb_probabilities = best_xgb_model.predict_proba(X_test)[:, 1]  # Extract probability for positive class

# Compute Confusion Matrix
conf_matrix_xgb = confusion_matrix(y_test, xgb_predictions)

# Compute Performance Metrics
accuracyXGB = round(accuracy_score(y_test, xgb_predictions), 3)
precisionXGB = round(precision_score(y_test, xgb_predictions), 3)
recallXGB = round(recall_score(y_test, xgb_predictions), 3)
f1_scoreXGB = round(f1_score(y_test, xgb_predictions), 3)

# Compute ROC Curve and AUC Score
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probabilities)
auc_value_XGB = round(auc(fpr_xgb, tpr_xgb), 3)
```

```{python}
#| label: fig-XGB-Vis
#| fig-cap: "XGBoost Model Visualizations"

# --- XGBoost: Feature Importances ---
feature_importances_xgb = best_xgb_model.feature_importances_
importance_df_xgb = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances_xgb
}).sort_values(by='Importance', ascending=False)

conf_matrix_xgb = pd.DataFrame(conf_matrix_xgb, index=["No Default", "Default"],
                           columns=["No Default", "Default"])

 # --- Create Grid Layout ---
fig = plt.figure(constrained_layout=True, figsize=(10, 10))
gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1])

# Row 3 Col 1: ROC Curve (XGBoost)
ax0 = fig.add_subplot(gs[1, :])
ax0.plot(fpr_xgb, tpr_xgb, color="blue", linewidth=2, label=f"AUC: {auc_value_XGB:.2f}")
ax0.plot([0, 1], [0, 1], linestyle="--", color="red")
ax0.set_xlabel("False Positive Rate (1 - Specificity)")
ax0.set_ylabel("True Positive Rate (Sensitivity)")
ax0.set_title("(a): ROC Curve")
ax0.legend()

# Row 3 Col 2: Confusion Matrix (XGBoost)
ax1 = fig.add_subplot(gs[2, 0])
sns.heatmap(conf_matrix_xgb, annot=True, fmt="d", cmap="Blues", linewidths=0.5, cbar=False, annot_kws={"size": 18}, ax=ax1)
ax1.set_xlabel("Predicted Class")
ax1.set_ylabel("Actual Class")
ax1.set_title(" (b): Confusion Matrix ")

# Row 2 Col 2: Feature Importances (Random Forest)
ax2 = fig.add_subplot(gs[2, 1])
sns.barplot(x='Importance', y='Feature', data=importance_df_xgb,
            palette=sns.color_palette("Blues", n_colors=len(importance_df_xgb)),
            edgecolor='black', ax=ax2)
ax2.set_xlabel("Importance")
ax2.set_ylabel("Feature")
ax2.set_title("(c): Feature Importances")

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()


TN_xgb = conf_matrix_xgb.iloc[0, 0]  # True Negatives (No Default correctly classified)
FP_xgb = conf_matrix_xgb.iloc[0, 1]  # False Positives (Non-default misclassified as Default)
FN_xgb = conf_matrix_xgb.iloc[1, 0]  # False Negatives (Default misclassified as Non-default)
TP_xgb = conf_matrix_xgb.iloc[1, 1]  # True Positives (Default correctly classified)

top_features_xgb = importance_df_xgb.nlargest(3, 'Importance')['Feature'].tolist()  # Random Forest
Feature_1 = top_features_xgb[2]
Feature_2 = top_features_xgb[1]
Feature_3 = top_features_xgb[0]

imp_Feature_1 = importance_df_xgb.iloc[0,1].round(3)
imp_Feature_2 = importance_df_xgb.iloc[1,1].round(3)
imp_Feature_3 = importance_df_xgb.iloc[2,1].round(3)

```

Figure 7(a) visualises the performance of the XGBoost in classifying positive outcomes. This model achieves a slightly higher AUC score than the RF (AUC = `{python} str(auc_value_XGB)`), demonstrating excellent predictive performance (Çorbacıoğlu, 2023). Its built-in regularisation, max_depth, min_child_weight, and L1/L2 regularisation, reduces the risk of overfitting despite its complexity.

Figure 7(b) reiterates the increased performance of the XGBoost. XGBoost predicts `{python} str(TP_xgb - TP_rf)` more true positives than the RF, indicating better sensitivity avoiding potentially revenue losses due to defaulting. The model also has `{python} str(FP_xgb - FP_rf)` fewer false positives, meaning it incorrectly predicts fewer non-defaulters as defaulters.

Figure 7(c) contradicts the other models (LR and RF), as these models predicted *`{python} Feature_1`* to have less of an impact on the predictions than the XGBoost model. However, similar to the RF and LR model, the XGBoost placed high importance on *`{python} Feature_2`* reinforcing the notion that the proportion of income allocated to a loan significantly impacts the risk of defaulting. However, *`{python} Feature_3`* ranks higher than in the RF and LR, indicating that home ownership status may play a larger role in how XGBoosts evaluates credit risk.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

### **3.4 Model Evaluation and Comparisons**

```{python}
#| label: fig-model-eval
#| fig-cap: "Model Evaluation and Comparisons"

# Log Loss
log_loss_LR = round(log_loss(y_test, lr_predictions), 3)
log_loss_RF = round(log_loss(y_test, rf_predictions), 3)
log_loss_XGB = round(log_loss(y_test, xgb_predictions), 3)

# Brier Scores
brier_LR = round(brier_score_loss(y_test, lr_predictions),3)
brier_RF = round(brier_score_loss(y_test, rf_predictions),3)
brier_XGB = round(brier_score_loss(y_test, xgb_predictions),3)

# Create a DataFrame for Performance Metrics
performance_df = pd.DataFrame({
    "Model": ["LR","RF", "XGB"],
    "Accuracy": [accuracyLR,accuracyRF, accuracyXGB],
    "Precision": [precisionLR,precisionRF, precisionXGB],
    "Sensitivity": [recallLR,recallRF, recallXGB],
    "F1 Score": [f1_scoreLR,f1_scoreRF, f1_scoreXGB],
    "AUC": [auc_value_LR,auc_value_RF, auc_value_XGB],
    "Log Loss": [log_loss_LR,log_loss_RF, log_loss_XGB],
    "Brier Score": [brier_LR,brier_RF, brier_XGB]
})

# Extract top 3 features for each model
top_features_lr = conf.index[-3:].tolist()  # Logistic Regression (based on Odds Ratios)
top_features_rf = importance_df_rf.nlargest(3, 'Importance')['Feature'].tolist()  # Random Forest
top_features_xgb = importance_df_xgb.nlargest(3, 'Importance')['Feature'].tolist()  # XGBoost

# Create a DataFrame for top features
top_features_df = pd.DataFrame({
    "Logistic Regression": top_features_lr,
    "Random Forest": top_features_rf,
    "XGBoost": top_features_xgb
    }, index=["Feature 1", "Feature 2", "Feature 3"])  # Rename row labels

fig, axes = plt.subplots(3, 1, figsize=(8, 8), gridspec_kw={'height_ratios': [3, 1, 1]})

# --- Bar Plot (Accuracy) ---
sns.barplot(x='Model', y='Accuracy', data=performance_df, palette='Blues_d', ax=axes[0])
for i, acc in enumerate(performance_df['Accuracy']):
    axes[0].text(i, acc + 0.015, f"{acc:.3f}", ha='center', fontsize=10)
axes[0].set_ylim(0, 1)
axes[0].set_ylabel("Accuracy")
axes[0].set_title("(a): Accuracy for Each Model", fontsize=12)

# --- Metric Table ---
axes[1].axis('off')
table1 = axes[1].table(cellText=performance_df.round(3).values,
                       colLabels=performance_df.columns,
                       cellLoc='center', loc='center')
table1.auto_set_font_size(False)
table1.set_fontsize(10)
table1.scale(1.15, 1.4)
axes[1].set_title("(b): Performance Metrics for Each Model", fontsize=12)

# Bold headers
for (row, col), cell in table1.get_celld().items():
    if row == 0:
        cell.set_text_props(weight='bold')

# --- Top Features Table ---
axes[2].axis('off')
table2 = axes[2].table(cellText=top_features_df.values,
                       colLabels=top_features_df.columns,
                       loc='center', cellLoc='center')
table2.auto_set_font_size(False)
table2.set_fontsize(10)
table2.scale(1.15, 1.4)
axes[2].set_title("(c): Top 3 Most Important Features for Each Model", fontsize=12)

# Bold column headers
for (row, col), cell in table2.get_celld().items():
    if row == 0:
        cell.set_text_props(weight='bold')

# Final adjustments
plt.tight_layout(pad=1.5, rect=[0, 0, 1, 1])  # tighter fit
plt.subplots_adjust(hspace=0.4)
plt.show()
```

Figure 8(a) shows that XGBoost (`{python} str(accuracyXGB)`) achieves the highest accuracy, closely followed by Random Forest (`{python} str(accuracyRF)`), while Logistic Regression lags (`{python} str(accuracyLR)`). However, accuracy alone can be misleading in imbalanced classification tasks such as credit scoring, where minimizing the cost of misclassifying defaulters is critical (Hand & Henley, 1997)

Figure 8(b) highlights XGBoost’s overall strength, with the highest AUC (`{python} str(auc_value_XGB)`), strong sensitivity (`{python} str(recallXGB)`), and F1 score (`{python} str(f1_scoreXGB)`). RF performs similarly, though with slightly lower recall. LR underperforms across most metrics except sensitivity (`{python} str (recallLR)`) however, precision (`{python} str (precisionLR)`) is lacking reducing the applications of this model. Also LR has, the highest log loss (`{python} str(log_loss_LR)`), suggesting poorer probability calibration.

In credit risk analysis, recall is critical—missing a default is costlier than a false alarm (Hand & Henley, 1997). While LR has competitive recall, XGBoost offers a better balance between recall, precision, and overall performance.

Table 8(c) shows LR focuses on income-related variables, suggesting linear assumptions. Tree-based models prioritize features like LoanGrade and HomeOwnership, capturing non-linear interactions more effectively (Zhou et al., 2002), making them more suited to complex credit risk tasks.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

### **3.5 Who Gets a Fair Deal? Auditing XGBoost’s Default Predictions**

```{python}

# Merge unprocessed data into X_test
X_test_fair = X_test.copy()
X_test_fair['y_true'] = y_test
X_test_fair['y_pred'] = xgb_predictions

# Attach raw (unscaled, unencoded) variables from loan_data_pre
X_test_fair['PersonIncome'] = loan_data_pre.loc[X_test.index, 'PersonIncome']
X_test_fair['PersonAge'] = loan_data_pre.loc[X_test.index, 'PersonAge']
X_test_fair['PersonHomeOwnership'] = loan_data_pre.loc[X_test.index, 'PersonHomeOwnership']

# Define median thresholds for age/income split
income_threshold = X_test_fair['PersonIncome'].median()
age_threshold = X_test_fair['PersonAge'].median()

# Define subgroups
low_income = X_test_fair[X_test_fair['PersonIncome'] < income_threshold]
high_income = X_test_fair[X_test_fair['PersonIncome'] >= income_threshold]
young = X_test_fair[X_test_fair['PersonAge'] < age_threshold]
old = X_test_fair[X_test_fair['PersonAge'] >= age_threshold]
renters = X_test_fair[X_test_fair['PersonHomeOwnership'] == 'RENT']
owners = X_test_fair[X_test_fair['PersonHomeOwnership'] == 'OWN']
mortgagers = X_test_fair[X_test_fair['PersonHomeOwnership'] == 'MORTGAGE']


# Function to calculate metrics
def subgroup_metrics(group_name, y_true, y_pred):
    return {
        'Group': group_name,
        'Accuracy': round(accuracy_score(y_true, y_pred), 3),
        'Recall': round(recall_score(y_true, y_pred), 3),
        'F1': round(f1_score(y_true, y_pred), 3)
    }

# Compile results
fairness_results = pd.DataFrame([
    subgroup_metrics("Low Income", low_income['y_true'], low_income['y_pred']),
    subgroup_metrics("High Income", high_income['y_true'], high_income['y_pred']),
    subgroup_metrics("Young (< median age)", young['y_true'], young['y_pred']),
    subgroup_metrics("Old (≥ median age)", old['y_true'], old['y_pred']),
    subgroup_metrics("Renters", renters['y_true'], renters['y_pred']),
    subgroup_metrics("Homeowners", owners['y_true'], owners['y_pred']),
    subgroup_metrics("Mortgage", mortgagers['y_true'], mortgagers['y_pred']),

])

# Display in Quarto

# Extract recall values for specific subgroups
recall_low_income = fairness_results.loc[fairness_results['Group'] == "Low Income", 'Recall'].values[0]
recall_high_income = fairness_results.loc[fairness_results['Group'] == "High Income", 'Recall'].values[0]
recall_renters = fairness_results.loc[fairness_results['Group'] == "Renters", 'Recall'].values[0]
recall_homeowners = fairness_results.loc[fairness_results['Group'] == "Homeowners", 'Recall'].values[0]
recall_mortgagers = fairness_results.loc[fairness_results['Group'] == "Mortgage", 'Recall'].values[0]

# Also grab F1 for mortgagers if needed
f1_mortgagers = fairness_results.loc[fairness_results['Group'] == "Mortgage", 'F1'].values[0]
```

```{python}
#| tbl-cap: "Table 2: Fairness Audit"

HTML(fairness_results.to_html(index=False))
```

The fairness audit of the XGBoost model highlights clear differences in recall across borrower subgroups. While overall accuracy remains high, recall is substantially lower for high-income (`{python} str(recall_high_income)`) and mortgage (`{python} str(recall_mortgagers)`) groups, suggesting the model is less effective at flagging defaulters in these categories. In contrast, renters (`{python} str(recall_renters)`) and homeowners (`{python} str(recall_homeowners)`) benefit from stronger sensitivity. These gaps may reflect imbalanced data or overlapping traits that obscure risk signals—for instance, mortgagers may combine stable income with high debt, complicating predictions. High-income defaulters may also be underrepresented, reducing model focus. Addressing these differences through subgroup-aware tuning or fairness constraints can improve equity in credit assessments.

### **3.6 Practical Implications and Limitations**

The project found that ensemble learning models, specifically XGBoost and RF, improve credit risk assessment. These models outperform traditional methods because they detect complex patterns in financial data and deal with imbalanced datasets more effectively (Chopra & Bhilare, 2018). Lenders can improve their risk assessment frameworks by focussing on key predictors such as *LoanGrade*, *LoanPercentIncome*, and *PersonIncome*. This will increase accuracy and reduce misclassification errors. This leads to more informed lending decisions, lower default rates, and improved overall portfolio performance.

However, ensemble models present several challenges. Their complexity reduces interpretability, which is a key consideration in regulated industries where transparency is required (Afolabi, 2024). Ensemble models require higher computational resources, increasing costs and making them less accessible to smaller institutions (Lei, 2025). Without proper tuning, these models risk overfitting on imbalanced data (Cheng et al., 2021) and may amplify existing biases, raising fairness concerns in lending (Shah & Davis, 2025). Addressing these limitations requires further research into explainable AI, optimised model tuning, and bias mitigation strategies to ensure fair and reliable predictions.

<hr style="width: 150px; border: none; height: 4px; background-color:rgb(45,62,81,255); margin: auto;">

## **4. Key Takeaways**

This project evaluated the performance of LR, RF, and XGBoost in predicting loan defaults. XGB achieved the best results across all metrics — accuracy (`{python} str(accuracyXGB)`), AUC (`{python} str(auc_value_XGB)`), and F1-score (`{python} str(f1_scoreXGB)`) — showing its ability to capture complex, non-linear patterns in borrower behavior. RF also performed well, although its slightly lower sensitivity suggests it may miss more defaulters. LR lagged behind due to its linear assumptions, making it less suitable for this problem.

Feature importance analysis showed that LR prioritised income-based features like PersonIncome and LoanPercentIncome, whereas RF and XGB placed greater emphasis on LoanGrade, and PersonHomeOwnership — indicating that ensemble models take a more holistic view of credit risk.

To understand variation across borrower profiles, a causal forest was used to estimate Conditional Average Treatment Effects (CATEs) for PreviousDefault. Most values clustered around zero, but a distinct subgroup had significantly higher CATEs, indicating a sharply increased likelihood of defaulting again after a previous default. This demonstrates the value of causal inference in highlighting heterogeneity that average models may miss.

While powerful, ensemble models are harder to interpret and require tuning. Future improvements could focus on explainability and fairness to support real-world deployment. The fairness audit highlighted that even top-performing models can yield uneven results across demographic groups, reinforcing the importance of subgroup evaluation in credit decision-making pipelines

[Link to Github Repository = BEE2041 Data Science In Economics Empirical Project](https://github.com/JoshLG18/DSE-EMP-Project)